{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预览和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"./input/poetryFromTang.txt\",\"r\",encoding='utf-8')\n",
    "poetry_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n巴山上峡重复重，阳台碧峭十二峰。荆王猎时逢暮雨，\\n夜卧高丘梦神女。轻红流烟湿艳姿，行云飞去明星稀。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16647"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poetry_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 巴山上峡重复重 阳台碧峭十二峰 荆王猎时逢暮雨  夜卧高丘梦神女 轻红流烟湿艳姿 行云飞去明星稀  目极魂断望不见 猿啼三声泪沾衣   见尽数万里 不闻三声猿 但飞萧萧雨 中有亭亭魂  千载楚襄恨 '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus = poetry_corpus.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "poetry_corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_features = 10000\n",
    "vocab = set(poetry_corpus)\n",
    "vocab_dict={}\n",
    "for word in poetry_corpus:\n",
    "    if word in vocab_dict:\n",
    "        vocab_dict[word] +=1\n",
    "    else :\n",
    "        vocab_dict[word] = 1\n",
    "vocab_list = []\n",
    "for word in vocab_dict:\n",
    "    vocab_list.append((word, vocab_dict[word]))\n",
    "vocab_list.sort(key=lambda x: x[1], reverse=True)\n",
    "if len(vocab_list) > max_features:\n",
    "    vocab_list = vocab_list[:max_features]\n",
    "vocab = [x[0] for x in vocab_list]\n",
    "word_to_idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx_to_word = dict(enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_arr(text):\n",
    "    \n",
    "    arr = []\n",
    "    for word in text:\n",
    "        if word in word_to_idx:\n",
    "            arr.append(word_to_idx[word])\n",
    "        else: arr.append(len(vocab))\n",
    "    return np.array(arr)\n",
    "def arr_to_text(arr):\n",
    "    words = []\n",
    "    for index in arr:\n",
    "        if index == len(vocab):\n",
    "            words.append('<unk>')\n",
    "        elif index < len(vocab):\n",
    "            words.append(idx_to_word[index])\n",
    "        else:\n",
    "            raise Exception('Unknown index!')\n",
    "    return \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  921,   10,   28, 1201,  275,   56,  275,    0,  110,  128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_arr = poetry_corpus[:11]\n",
    "text_to_arr(text_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造文本时序数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#序列长度\n",
    "sequence = 5\n",
    "# 总的序列个数\n",
    "num_seq = int(len(poetry_corpus) / sequence)\n",
    "text = poetry_corpus[:num_seq*sequence]\n",
    "num_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整为Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "arr = text_to_arr(text)\n",
    "arr = arr.reshape((num_seq, -1))\n",
    "arr = torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(object):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        x = self.arr[item, :]\n",
    "        \n",
    "        # 构造 label\n",
    "        y = torch.zeros(x.shape)\n",
    "        # 将输入的第一个字符作为最后一个输入的 label\n",
    "        y[:-1], y[-1] = x[1:], x[0]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TextDataset at 0x174c7526d68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 巴山上峡\n",
      "巴山上峡 \n"
     ]
    }
   ],
   "source": [
    "x, y = train_set[0]\n",
    "print(arr_to_text(x.numpy()))\n",
    "print(arr_to_text(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class char_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dim, hidden_state, embedding_matrix=None):\n",
    "        super(char_GRU, self).__init__()\n",
    "        #嵌入层\n",
    "        self.num_layers = vocab_size\n",
    "        self.hidden_size = hidden_state\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        if embedding_matrix is not None:\n",
    "            #加载词向量\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            #冻结参数\n",
    "            #self.embedding.weight.data.requires_grad = False\n",
    "        self.rnn = nn.GRU(embedding_dim,hidden_state,2,batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_state, vocab_size)\n",
    "\n",
    "    def forward(self, x,hs=None):\n",
    "        batch=x.shape[0]\n",
    "        embed = self.embedding(x) \n",
    "        if hs is None:\n",
    "            hs = Variable(\n",
    "                torch.zeros(self.num_layers, batch, self.hidden_size))\n",
    "        out,hs= self.rnn(embed)\n",
    "        out = self.fc(out)\n",
    "        return out.view(-1,vocab_size),hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "vocab_size = len(vocab)+1\n",
    "train_data = DataLoader(train_set, batch_size, True)\n",
    "model = char_GRU(vocab_size, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, perplexity is: 945.885\n",
      "epoch: 2, perplexity is: 572.340\n",
      "epoch: 3, perplexity is: 521.260\n",
      "epoch: 4, perplexity is: 401.698\n",
      "epoch: 5, perplexity is: 275.885\n",
      "epoch: 6, perplexity is: 177.950\n",
      "epoch: 7, perplexity is: 107.476\n",
      "epoch: 8, perplexity is: 79.147\n",
      "epoch: 9, perplexity is: 55.702\n",
      "epoch: 10, perplexity is: 41.112\n",
      "epoch: 11, perplexity is: 29.948\n",
      "epoch: 12, perplexity is: 25.140\n",
      "epoch: 13, perplexity is: 21.323\n",
      "epoch: 14, perplexity is: 18.985\n",
      "epoch: 15, perplexity is: 16.399\n",
      "epoch: 16, perplexity is: 14.243\n",
      "epoch: 17, perplexity is: 12.725\n",
      "epoch: 18, perplexity is: 12.243\n",
      "epoch: 19, perplexity is: 9.589\n",
      "epoch: 20, perplexity is: 8.733\n",
      "epoch: 21, perplexity is: 8.724\n",
      "epoch: 22, perplexity is: 7.560\n",
      "epoch: 23, perplexity is: 6.959\n",
      "epoch: 24, perplexity is: 6.771\n",
      "epoch: 25, perplexity is: 5.751\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 25\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    for data in train_data:\n",
    "        x, y = data\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        # Forward.\n",
    "        score,_ = model(x)\n",
    "        loss= criterion(score, y.view(-1))\n",
    "\n",
    "        # Backward.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print('epoch: {}, perplexity is: {:.3f}'.format(e+1, np.exp(train_loss / len(train_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在概率最高的几个字符，随机选取一个作为输出\n",
    "def pick_top_n(preds, top_n=5):\n",
    "    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n",
    "    top_pred_prob /= torch.sum(top_pred_prob)\n",
    "    top_pred_prob = top_pred_prob.squeeze(0).cpu().numpy()\n",
    "    top_pred_label = top_pred_label.squeeze(0).cpu().numpy()\n",
    "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夜卧高丘梦神神知妾天漏下下南 隅寒蒲城阙百衔树临临临流萦玉薄过楼楼珠争玉\n"
     ]
    }
   ],
   "source": [
    "begin = '夜卧高丘梦神'\n",
    "text_len = 30\n",
    "\n",
    "model = model.eval()\n",
    "samples = [word_to_idx[c] for c in begin]\n",
    "input_txt = torch.LongTensor(samples)[None]\n",
    "input_txt = Variable(input_txt)\n",
    "_, init_state = model(input_txt)\n",
    "result = samples\n",
    "model_input = input_txt[:, -1][:, None]\n",
    "for i in range(text_len):\n",
    "    out, init_state = model(model_input, init_state)\n",
    "    pred = pick_top_n(out.data)\n",
    "    model_input = Variable(torch.LongTensor(pred))[None]\n",
    "    result.append(pred[0])\n",
    "text = arr_to_text(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class char_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_dim, hidden_state, embedding_matrix=None):\n",
    "        super(char_LSTM, self).__init__()\n",
    "        #嵌入层\n",
    "        self.num_layers = vocab_size\n",
    "        self.hidden_size = hidden_state\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
    "        if embedding_matrix is not None:\n",
    "            #加载词向量\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            #冻结参数\n",
    "            #self.embedding.weight.data.requires_grad = False\n",
    "        self.rnn = nn.LSTM(embedding_dim,hidden_state,2,batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_state, vocab_size)\n",
    "\n",
    "    def forward(self, x,hn=None):\n",
    "        batch=x.shape[0]\n",
    "        embed = self.embedding(x) \n",
    "        out,hn= self.rnn(embed,hn)\n",
    "        out = self.fc(out)\n",
    "        return out.view(-1,vocab_size),hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "vocab_size = len(vocab)+1\n",
    "train_data = DataLoader(train_set, batch_size, True)\n",
    "model = char_LSTM(vocab_size, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, perplexity is: 979.842\n",
      "epoch: 2, perplexity is: 544.762\n",
      "epoch: 3, perplexity is: 463.647\n",
      "epoch: 4, perplexity is: 447.757\n",
      "epoch: 5, perplexity is: 388.332\n",
      "epoch: 6, perplexity is: 330.095\n",
      "epoch: 7, perplexity is: 257.956\n",
      "epoch: 8, perplexity is: 208.875\n",
      "epoch: 9, perplexity is: 177.638\n",
      "epoch: 10, perplexity is: 141.000\n",
      "epoch: 11, perplexity is: 109.061\n",
      "epoch: 12, perplexity is: 85.458\n",
      "epoch: 13, perplexity is: 64.365\n",
      "epoch: 14, perplexity is: 46.743\n",
      "epoch: 15, perplexity is: 32.446\n",
      "epoch: 16, perplexity is: 26.736\n",
      "epoch: 17, perplexity is: 19.438\n",
      "epoch: 18, perplexity is: 15.259\n",
      "epoch: 19, perplexity is: 12.001\n",
      "epoch: 20, perplexity is: 9.881\n",
      "epoch: 21, perplexity is: 8.350\n",
      "epoch: 22, perplexity is: 6.784\n",
      "epoch: 23, perplexity is: 5.939\n",
      "epoch: 24, perplexity is: 4.902\n",
      "epoch: 25, perplexity is: 4.203\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 25\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    for data in train_data:\n",
    "        x, y = data\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        # Forward.\n",
    "        score,_ = model(x)\n",
    "        loss= criterion(score, y.view(-1))\n",
    "\n",
    "        # Backward.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    print('epoch: {}, perplexity is: {:.3f}'.format(e+1, np.exp(train_loss / len(train_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荆王猎时逢暮何古人世问平白草平知 日世事平朝 时东开 平朝云平十高高十古\n"
     ]
    }
   ],
   "source": [
    "begin = '荆王猎时逢暮'\n",
    "text_len = 30\n",
    "\n",
    "model = model.eval()\n",
    "samples = [word_to_idx[c] for c in begin]\n",
    "input_txt = torch.LongTensor(samples)[None]\n",
    "input_txt = Variable(input_txt)\n",
    "_, init_state = model(input_txt)\n",
    "result = samples\n",
    "model_input = input_txt[:, -1][:, None]\n",
    "for i in range(text_len):\n",
    "    out, init_state = model(model_input, init_state)\n",
    "    pred = pick_top_n(out.data)\n",
    "    model_input = Variable(torch.LongTensor(pred))[None]\n",
    "    result.append(pred[0])\n",
    "text = arr_to_text(result)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

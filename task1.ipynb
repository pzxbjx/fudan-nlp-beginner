{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train=pd.read_csv('./input/train.tsv', sep='\\t')\n",
    "test=pd.read_csv('./input/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def clean_data(text):\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \" \",text)\n",
    "    text = re.sub(\"-\",\" \",text)\n",
    "    tokens = text.lower().split()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return \" \".join(filtered_tokens)\n",
    "train_data = train['Phrase'].apply(clean_data)\n",
    "test_data = test['Phrase'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "\n",
    "class Ngram():\n",
    "    def __init__(self, max_features=None, ngram=1):\n",
    "        self.max_features = max_features #最大特征数\n",
    "        self.ngram = ngram\n",
    "\n",
    "    def getngrams(self,text, n):\n",
    "        output = {}\n",
    "        for sentence in text:\n",
    "            tokens = sentence.split()\n",
    "            for i in range(len(tokens)+1-n):  #遍历查找n元组\n",
    "                ngramTemp = \" \".join(tokens[i:i+n])\n",
    "                if ngramTemp in output:\n",
    "                    output[ngramTemp] += 1\n",
    "                else: output[ngramTemp] = 0        #统计词频\n",
    "        return output\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.vocabulary_ = {}\n",
    "        self.vocabulary_.update(self.getngrams(data,self.ngram))   #构造词表\n",
    "        if self.max_features:\n",
    "            #排序，然后保留前max_features个词组\n",
    "            vocab = sorted(self.vocabulary_.items(),key = lambda x:x[1],reverse = True)\n",
    "            for key, value in vocab[self.max_features:len(self.vocabulary_)]:\n",
    "                del self.vocabulary_[str(key)]\n",
    "        #编码\n",
    "        label = 0\n",
    "        for i in self.vocabulary_.keys():\n",
    "            self.vocabulary_[i] = label\n",
    "            label += 1\n",
    "    \n",
    "    def transform(self, data):\n",
    "        array = np.zeros([len(data),len(self.vocabulary_)])\n",
    "        n = len(data)\n",
    "        for i in range(n):\n",
    "            tokens = data[i].split()\n",
    "            for j in range(len(tokens)+1-self.ngram):     #遍历查找n元组\n",
    "                ngramTemp = \" \".join(tokens[j:j+self.ngram])\n",
    "                if ngramTemp in self.vocabulary_.keys():\n",
    "                    array[i][self.vocabulary_[ngramTemp]] = 1\n",
    "\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text= np.zeros((len(train_data),3000))\n",
    "test_text = np.zeros((len(test_data),3000))\n",
    "for i in range(1,4):\n",
    "    vec = Ngram(1000, i)\n",
    "    vec.fit(list(train_data)+list(test_data))\n",
    "    train_text[:,1000*(i-1):1000*i] = vec.transform(train_data)\n",
    "    test_text[:,1000*(i-1):1000*i] = vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "tmp = list(range(0,len(train_data)))\n",
    "mask = random.sample(tmp, 16060)\n",
    "val_text = train_text[mask]\n",
    "train_text = np.delete(train_text,mask,axis=0)\n",
    "y = np.array(train['Sentiment'])\n",
    "y_val = y[mask]\n",
    "y_train = np.delete(y,mask,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax(cross entropy loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 4000: loss 1.608666\n",
      "iteration 100 / 4000: loss 1.271652\n",
      "iteration 200 / 4000: loss 1.166203\n",
      "iteration 300 / 4000: loss 1.237849\n",
      "iteration 400 / 4000: loss 1.193581\n",
      "iteration 500 / 4000: loss 1.241890\n",
      "iteration 600 / 4000: loss 1.151648\n",
      "iteration 700 / 4000: loss 1.185860\n",
      "iteration 800 / 4000: loss 1.299731\n",
      "iteration 900 / 4000: loss 1.239409\n",
      "iteration 1000 / 4000: loss 1.167822\n",
      "iteration 1100 / 4000: loss 1.157637\n",
      "iteration 1200 / 4000: loss 1.092439\n",
      "iteration 1300 / 4000: loss 1.222840\n",
      "iteration 1400 / 4000: loss 1.213583\n",
      "iteration 1500 / 4000: loss 1.208075\n",
      "iteration 1600 / 4000: loss 1.166120\n",
      "iteration 1700 / 4000: loss 1.167774\n",
      "iteration 1800 / 4000: loss 1.125052\n",
      "iteration 1900 / 4000: loss 1.188323\n",
      "iteration 2000 / 4000: loss 1.205866\n",
      "iteration 2100 / 4000: loss 1.214412\n",
      "iteration 2200 / 4000: loss 1.082307\n",
      "iteration 2300 / 4000: loss 1.231721\n",
      "iteration 2400 / 4000: loss 1.181714\n",
      "iteration 2500 / 4000: loss 1.128529\n",
      "iteration 2600 / 4000: loss 1.150695\n",
      "iteration 2700 / 4000: loss 1.122306\n",
      "iteration 2800 / 4000: loss 1.131091\n",
      "iteration 2900 / 4000: loss 1.106122\n",
      "iteration 3000 / 4000: loss 1.165824\n",
      "iteration 3100 / 4000: loss 1.098290\n",
      "iteration 3200 / 4000: loss 1.193867\n",
      "iteration 3300 / 4000: loss 1.098064\n",
      "iteration 3400 / 4000: loss 1.156140\n",
      "iteration 3500 / 4000: loss 1.208016\n",
      "iteration 3600 / 4000: loss 1.167437\n",
      "iteration 3700 / 4000: loss 1.123707\n",
      "iteration 3800 / 4000: loss 1.077866\n",
      "iteration 3900 / 4000: loss 1.170131\n",
      "iteration 0 / 4000: loss 1.609118\n",
      "iteration 100 / 4000: loss 1.395448\n",
      "iteration 200 / 4000: loss 1.336337\n",
      "iteration 300 / 4000: loss 1.272633\n",
      "iteration 400 / 4000: loss 1.323615\n",
      "iteration 500 / 4000: loss 1.253911\n",
      "iteration 600 / 4000: loss 1.148797\n",
      "iteration 700 / 4000: loss 1.187841\n",
      "iteration 800 / 4000: loss 1.279537\n",
      "iteration 900 / 4000: loss 1.268603\n",
      "iteration 1000 / 4000: loss 1.277906\n",
      "iteration 1100 / 4000: loss 1.231893\n",
      "iteration 1200 / 4000: loss 1.244381\n",
      "iteration 1300 / 4000: loss 1.209732\n",
      "iteration 1400 / 4000: loss 1.297234\n",
      "iteration 1500 / 4000: loss 1.191441\n",
      "iteration 1600 / 4000: loss 1.187192\n",
      "iteration 1700 / 4000: loss 1.227426\n",
      "iteration 1800 / 4000: loss 1.190446\n",
      "iteration 1900 / 4000: loss 1.173613\n",
      "iteration 2000 / 4000: loss 1.268282\n",
      "iteration 2100 / 4000: loss 1.147657\n",
      "iteration 2200 / 4000: loss 1.235344\n",
      "iteration 2300 / 4000: loss 1.265287\n",
      "iteration 2400 / 4000: loss 1.151617\n",
      "iteration 2500 / 4000: loss 1.243746\n",
      "iteration 2600 / 4000: loss 1.143519\n",
      "iteration 2700 / 4000: loss 1.223145\n",
      "iteration 2800 / 4000: loss 1.201165\n",
      "iteration 2900 / 4000: loss 1.186820\n",
      "iteration 3000 / 4000: loss 1.223288\n",
      "iteration 3100 / 4000: loss 1.208326\n",
      "iteration 3200 / 4000: loss 1.169108\n",
      "iteration 3300 / 4000: loss 1.245772\n",
      "iteration 3400 / 4000: loss 1.160299\n",
      "iteration 3500 / 4000: loss 1.084040\n",
      "iteration 3600 / 4000: loss 1.175985\n",
      "iteration 3700 / 4000: loss 1.176151\n",
      "iteration 3800 / 4000: loss 1.177849\n",
      "iteration 3900 / 4000: loss 1.128990\n",
      "iteration 0 / 4000: loss 1.609210\n",
      "iteration 100 / 4000: loss 1.481473\n",
      "iteration 200 / 4000: loss 1.431757\n",
      "iteration 300 / 4000: loss 1.355926\n",
      "iteration 400 / 4000: loss 1.357032\n",
      "iteration 500 / 4000: loss 1.335044\n",
      "iteration 600 / 4000: loss 1.270217\n",
      "iteration 700 / 4000: loss 1.286552\n",
      "iteration 800 / 4000: loss 1.344825\n",
      "iteration 900 / 4000: loss 1.218964\n",
      "iteration 1000 / 4000: loss 1.247112\n",
      "iteration 1100 / 4000: loss 1.280777\n",
      "iteration 1200 / 4000: loss 1.203583\n",
      "iteration 1300 / 4000: loss 1.293629\n",
      "iteration 1400 / 4000: loss 1.315096\n",
      "iteration 1500 / 4000: loss 1.150137\n",
      "iteration 1600 / 4000: loss 1.227387\n",
      "iteration 1700 / 4000: loss 1.264847\n",
      "iteration 1800 / 4000: loss 1.350694\n",
      "iteration 1900 / 4000: loss 1.198327\n",
      "iteration 2000 / 4000: loss 1.264831\n",
      "iteration 2100 / 4000: loss 1.314303\n",
      "iteration 2200 / 4000: loss 1.214550\n",
      "iteration 2300 / 4000: loss 1.312678\n",
      "iteration 2400 / 4000: loss 1.245905\n",
      "iteration 2500 / 4000: loss 1.242158\n",
      "iteration 2600 / 4000: loss 1.226005\n",
      "iteration 2700 / 4000: loss 1.286770\n",
      "iteration 2800 / 4000: loss 1.300130\n",
      "iteration 2900 / 4000: loss 1.341719\n",
      "iteration 3000 / 4000: loss 1.303887\n",
      "iteration 3100 / 4000: loss 1.292078\n",
      "iteration 3200 / 4000: loss 1.211131\n",
      "iteration 3300 / 4000: loss 1.284948\n",
      "iteration 3400 / 4000: loss 1.221665\n",
      "iteration 3500 / 4000: loss 1.244559\n",
      "iteration 3600 / 4000: loss 1.236093\n",
      "iteration 3700 / 4000: loss 1.134256\n",
      "iteration 3800 / 4000: loss 1.167743\n",
      "iteration 3900 / 4000: loss 1.269030\n",
      "iteration 0 / 4000: loss 1.609798\n",
      "iteration 100 / 4000: loss 1.563033\n",
      "iteration 200 / 4000: loss 1.529633\n",
      "iteration 300 / 4000: loss 1.488316\n",
      "iteration 400 / 4000: loss 1.487342\n",
      "iteration 500 / 4000: loss 1.445177\n",
      "iteration 600 / 4000: loss 1.466890\n",
      "iteration 700 / 4000: loss 1.351679\n",
      "iteration 800 / 4000: loss 1.387826\n",
      "iteration 900 / 4000: loss 1.389079\n",
      "iteration 1000 / 4000: loss 1.399882\n",
      "iteration 1100 / 4000: loss 1.363046\n",
      "iteration 1200 / 4000: loss 1.305798\n",
      "iteration 1300 / 4000: loss 1.342122\n",
      "iteration 1400 / 4000: loss 1.331427\n",
      "iteration 1500 / 4000: loss 1.364768\n",
      "iteration 1600 / 4000: loss 1.345281\n",
      "iteration 1700 / 4000: loss 1.350593\n",
      "iteration 1800 / 4000: loss 1.288561\n",
      "iteration 1900 / 4000: loss 1.294566\n",
      "iteration 2000 / 4000: loss 1.374492\n",
      "iteration 2100 / 4000: loss 1.307597\n",
      "iteration 2200 / 4000: loss 1.285163\n",
      "iteration 2300 / 4000: loss 1.311597\n",
      "iteration 2400 / 4000: loss 1.307293\n",
      "iteration 2500 / 4000: loss 1.357491\n",
      "iteration 2600 / 4000: loss 1.257590\n",
      "iteration 2700 / 4000: loss 1.285435\n",
      "iteration 2800 / 4000: loss 1.357163\n",
      "iteration 2900 / 4000: loss 1.337689\n",
      "iteration 3000 / 4000: loss 1.391488\n",
      "iteration 3100 / 4000: loss 1.319100\n",
      "iteration 3200 / 4000: loss 1.420423\n",
      "iteration 3300 / 4000: loss 1.276807\n",
      "iteration 3400 / 4000: loss 1.277854\n",
      "iteration 3500 / 4000: loss 1.274847\n",
      "iteration 3600 / 4000: loss 1.260464\n",
      "iteration 3700 / 4000: loss 1.260453\n",
      "iteration 3800 / 4000: loss 1.278152\n",
      "iteration 3900 / 4000: loss 1.196724\n",
      "iteration 0 / 4000: loss 1.609450\n",
      "iteration 100 / 4000: loss 1.591190\n",
      "iteration 200 / 4000: loss 1.578837\n",
      "iteration 300 / 4000: loss 1.562620\n",
      "iteration 400 / 4000: loss 1.551762\n",
      "iteration 500 / 4000: loss 1.531139\n",
      "iteration 600 / 4000: loss 1.518208\n",
      "iteration 700 / 4000: loss 1.512976\n",
      "iteration 800 / 4000: loss 1.506446\n",
      "iteration 900 / 4000: loss 1.509608\n",
      "iteration 1000 / 4000: loss 1.477745\n",
      "iteration 1100 / 4000: loss 1.488599\n",
      "iteration 1200 / 4000: loss 1.483039\n",
      "iteration 1300 / 4000: loss 1.429342\n",
      "iteration 1400 / 4000: loss 1.489945\n",
      "iteration 1500 / 4000: loss 1.433825\n",
      "iteration 1600 / 4000: loss 1.429011\n",
      "iteration 1700 / 4000: loss 1.419462\n",
      "iteration 1800 / 4000: loss 1.452595\n",
      "iteration 1900 / 4000: loss 1.406759\n",
      "iteration 2000 / 4000: loss 1.402093\n",
      "iteration 2100 / 4000: loss 1.383837\n",
      "iteration 2200 / 4000: loss 1.408379\n",
      "iteration 2300 / 4000: loss 1.382247\n",
      "iteration 2400 / 4000: loss 1.384360\n",
      "iteration 2500 / 4000: loss 1.378281\n",
      "iteration 2600 / 4000: loss 1.403268\n",
      "iteration 2700 / 4000: loss 1.344381\n",
      "iteration 2800 / 4000: loss 1.421526\n",
      "iteration 2900 / 4000: loss 1.360064\n",
      "iteration 3000 / 4000: loss 1.397650\n",
      "iteration 3100 / 4000: loss 1.328771\n",
      "iteration 3200 / 4000: loss 1.361383\n",
      "iteration 3300 / 4000: loss 1.414448\n",
      "iteration 3400 / 4000: loss 1.346841\n",
      "iteration 3500 / 4000: loss 1.305146\n",
      "iteration 3600 / 4000: loss 1.335362\n",
      "iteration 3700 / 4000: loss 1.354879\n",
      "iteration 3800 / 4000: loss 1.358067\n",
      "iteration 3900 / 4000: loss 1.353553\n",
      "iteration 0 / 4000: loss 1.609363\n",
      "iteration 100 / 4000: loss 1.243575\n",
      "iteration 200 / 4000: loss 1.267499\n",
      "iteration 300 / 4000: loss 1.244821\n",
      "iteration 400 / 4000: loss 1.159486\n",
      "iteration 500 / 4000: loss 1.297369\n",
      "iteration 600 / 4000: loss 1.231678\n",
      "iteration 700 / 4000: loss 1.160920\n",
      "iteration 800 / 4000: loss 1.319521\n",
      "iteration 900 / 4000: loss 1.212110\n",
      "iteration 1000 / 4000: loss 1.372324\n",
      "iteration 1100 / 4000: loss 1.224574\n",
      "iteration 1200 / 4000: loss 1.257352\n",
      "iteration 1300 / 4000: loss 1.307755\n",
      "iteration 1400 / 4000: loss 1.204802\n",
      "iteration 1500 / 4000: loss 1.174924\n",
      "iteration 1600 / 4000: loss 1.194008\n",
      "iteration 1700 / 4000: loss 1.291034\n",
      "iteration 1800 / 4000: loss 1.230373\n",
      "iteration 1900 / 4000: loss 1.276483\n",
      "iteration 2000 / 4000: loss 1.232691\n",
      "iteration 2100 / 4000: loss 1.229137\n",
      "iteration 2200 / 4000: loss 1.198757\n",
      "iteration 2300 / 4000: loss 1.301171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2400 / 4000: loss 1.240170\n",
      "iteration 2500 / 4000: loss 1.284151\n",
      "iteration 2600 / 4000: loss 1.323291\n",
      "iteration 2700 / 4000: loss 1.275909\n",
      "iteration 2800 / 4000: loss 1.245520\n",
      "iteration 2900 / 4000: loss 1.322413\n",
      "iteration 3000 / 4000: loss 1.316556\n",
      "iteration 3100 / 4000: loss 1.302858\n",
      "iteration 3200 / 4000: loss 1.256686\n",
      "iteration 3300 / 4000: loss 1.310374\n",
      "iteration 3400 / 4000: loss 1.327240\n",
      "iteration 3500 / 4000: loss 1.304642\n",
      "iteration 3600 / 4000: loss 1.209307\n",
      "iteration 3700 / 4000: loss 1.243529\n",
      "iteration 3800 / 4000: loss 1.190704\n",
      "iteration 3900 / 4000: loss 1.204877\n",
      "iteration 0 / 4000: loss 1.610005\n",
      "iteration 100 / 4000: loss 1.365530\n",
      "iteration 200 / 4000: loss 1.328890\n",
      "iteration 300 / 4000: loss 1.249497\n",
      "iteration 400 / 4000: loss 1.250387\n",
      "iteration 500 / 4000: loss 1.287183\n",
      "iteration 600 / 4000: loss 1.282721\n",
      "iteration 700 / 4000: loss 1.303103\n",
      "iteration 800 / 4000: loss 1.240164\n",
      "iteration 900 / 4000: loss 1.259386\n",
      "iteration 1000 / 4000: loss 1.337995\n",
      "iteration 1100 / 4000: loss 1.328596\n",
      "iteration 1200 / 4000: loss 1.362631\n",
      "iteration 1300 / 4000: loss 1.242503\n",
      "iteration 1400 / 4000: loss 1.248428\n",
      "iteration 1500 / 4000: loss 1.354738\n",
      "iteration 1600 / 4000: loss 1.206783\n",
      "iteration 1700 / 4000: loss 1.240751\n",
      "iteration 1800 / 4000: loss 1.253757\n",
      "iteration 1900 / 4000: loss 1.425720\n",
      "iteration 2000 / 4000: loss 1.300235\n",
      "iteration 2100 / 4000: loss 1.314792\n",
      "iteration 2200 / 4000: loss 1.338718\n",
      "iteration 2300 / 4000: loss 1.253185\n",
      "iteration 2400 / 4000: loss 1.172610\n",
      "iteration 2500 / 4000: loss 1.260431\n",
      "iteration 2600 / 4000: loss 1.346953\n",
      "iteration 2700 / 4000: loss 1.289824\n",
      "iteration 2800 / 4000: loss 1.155134\n",
      "iteration 2900 / 4000: loss 1.132693\n",
      "iteration 3000 / 4000: loss 1.216139\n",
      "iteration 3100 / 4000: loss 1.259271\n",
      "iteration 3200 / 4000: loss 1.308011\n",
      "iteration 3300 / 4000: loss 1.272423\n",
      "iteration 3400 / 4000: loss 1.251926\n",
      "iteration 3500 / 4000: loss 1.265969\n",
      "iteration 3600 / 4000: loss 1.248420\n",
      "iteration 3700 / 4000: loss 1.359053\n",
      "iteration 3800 / 4000: loss 1.194926\n",
      "iteration 3900 / 4000: loss 1.253142\n",
      "iteration 0 / 4000: loss 1.610274\n",
      "iteration 100 / 4000: loss 1.474504\n",
      "iteration 200 / 4000: loss 1.419284\n",
      "iteration 300 / 4000: loss 1.404080\n",
      "iteration 400 / 4000: loss 1.357542\n",
      "iteration 500 / 4000: loss 1.364973\n",
      "iteration 600 / 4000: loss 1.317391\n",
      "iteration 700 / 4000: loss 1.323465\n",
      "iteration 800 / 4000: loss 1.343361\n",
      "iteration 900 / 4000: loss 1.334863\n",
      "iteration 1000 / 4000: loss 1.265647\n",
      "iteration 1100 / 4000: loss 1.269454\n",
      "iteration 1200 / 4000: loss 1.253665\n",
      "iteration 1300 / 4000: loss 1.280877\n",
      "iteration 1400 / 4000: loss 1.347631\n",
      "iteration 1500 / 4000: loss 1.279761\n",
      "iteration 1600 / 4000: loss 1.322421\n",
      "iteration 1700 / 4000: loss 1.356276\n",
      "iteration 1800 / 4000: loss 1.323289\n",
      "iteration 1900 / 4000: loss 1.243554\n",
      "iteration 2000 / 4000: loss 1.252802\n",
      "iteration 2100 / 4000: loss 1.259586\n",
      "iteration 2200 / 4000: loss 1.353585\n",
      "iteration 2300 / 4000: loss 1.312118\n",
      "iteration 2400 / 4000: loss 1.217667\n",
      "iteration 2500 / 4000: loss 1.389625\n",
      "iteration 2600 / 4000: loss 1.232015\n",
      "iteration 2700 / 4000: loss 1.314919\n",
      "iteration 2800 / 4000: loss 1.279461\n",
      "iteration 2900 / 4000: loss 1.257890\n",
      "iteration 3000 / 4000: loss 1.230028\n",
      "iteration 3100 / 4000: loss 1.189194\n",
      "iteration 3200 / 4000: loss 1.320409\n",
      "iteration 3300 / 4000: loss 1.227899\n",
      "iteration 3400 / 4000: loss 1.330946\n",
      "iteration 3500 / 4000: loss 1.228788\n",
      "iteration 3600 / 4000: loss 1.215332\n",
      "iteration 3700 / 4000: loss 1.328949\n",
      "iteration 3800 / 4000: loss 1.310500\n",
      "iteration 3900 / 4000: loss 1.241404\n",
      "iteration 0 / 4000: loss 1.610076\n",
      "iteration 100 / 4000: loss 1.572114\n",
      "iteration 200 / 4000: loss 1.538014\n",
      "iteration 300 / 4000: loss 1.517459\n",
      "iteration 400 / 4000: loss 1.486769\n",
      "iteration 500 / 4000: loss 1.457228\n",
      "iteration 600 / 4000: loss 1.455794\n",
      "iteration 700 / 4000: loss 1.388705\n",
      "iteration 800 / 4000: loss 1.391940\n",
      "iteration 900 / 4000: loss 1.394719\n",
      "iteration 1000 / 4000: loss 1.348801\n",
      "iteration 1100 / 4000: loss 1.399853\n",
      "iteration 1200 / 4000: loss 1.342275\n",
      "iteration 1300 / 4000: loss 1.350914\n",
      "iteration 1400 / 4000: loss 1.358444\n",
      "iteration 1500 / 4000: loss 1.336539\n",
      "iteration 1600 / 4000: loss 1.342414\n",
      "iteration 1700 / 4000: loss 1.307484\n",
      "iteration 1800 / 4000: loss 1.219261\n",
      "iteration 1900 / 4000: loss 1.359072\n",
      "iteration 2000 / 4000: loss 1.287421\n",
      "iteration 2100 / 4000: loss 1.365579\n",
      "iteration 2200 / 4000: loss 1.261239\n",
      "iteration 2300 / 4000: loss 1.380389\n",
      "iteration 2400 / 4000: loss 1.291489\n",
      "iteration 2500 / 4000: loss 1.305218\n",
      "iteration 2600 / 4000: loss 1.313285\n",
      "iteration 2700 / 4000: loss 1.337549\n",
      "iteration 2800 / 4000: loss 1.279664\n",
      "iteration 2900 / 4000: loss 1.248684\n",
      "iteration 3000 / 4000: loss 1.319811\n",
      "iteration 3100 / 4000: loss 1.303366\n",
      "iteration 3200 / 4000: loss 1.309023\n",
      "iteration 3300 / 4000: loss 1.248504\n",
      "iteration 3400 / 4000: loss 1.303699\n",
      "iteration 3500 / 4000: loss 1.397069\n",
      "iteration 3600 / 4000: loss 1.305190\n",
      "iteration 3700 / 4000: loss 1.191954\n",
      "iteration 3800 / 4000: loss 1.351140\n",
      "iteration 3900 / 4000: loss 1.286368\n",
      "iteration 0 / 4000: loss 1.609424\n",
      "iteration 100 / 4000: loss 1.592978\n",
      "iteration 200 / 4000: loss 1.585723\n",
      "iteration 300 / 4000: loss 1.568737\n",
      "iteration 400 / 4000: loss 1.547804\n",
      "iteration 500 / 4000: loss 1.546247\n",
      "iteration 600 / 4000: loss 1.521725\n",
      "iteration 700 / 4000: loss 1.511783\n",
      "iteration 800 / 4000: loss 1.521203\n",
      "iteration 900 / 4000: loss 1.476717\n",
      "iteration 1000 / 4000: loss 1.509379\n",
      "iteration 1100 / 4000: loss 1.455883\n",
      "iteration 1200 / 4000: loss 1.456965\n",
      "iteration 1300 / 4000: loss 1.497491\n",
      "iteration 1400 / 4000: loss 1.463759\n",
      "iteration 1500 / 4000: loss 1.454191\n",
      "iteration 1600 / 4000: loss 1.445888\n",
      "iteration 1700 / 4000: loss 1.415280\n",
      "iteration 1800 / 4000: loss 1.465592\n",
      "iteration 1900 / 4000: loss 1.454881\n",
      "iteration 2000 / 4000: loss 1.424151\n",
      "iteration 2100 / 4000: loss 1.425923\n",
      "iteration 2200 / 4000: loss 1.442791\n",
      "iteration 2300 / 4000: loss 1.398723\n",
      "iteration 2400 / 4000: loss 1.399850\n",
      "iteration 2500 / 4000: loss 1.398076\n",
      "iteration 2600 / 4000: loss 1.390419\n",
      "iteration 2700 / 4000: loss 1.397246\n",
      "iteration 2800 / 4000: loss 1.369286\n",
      "iteration 2900 / 4000: loss 1.367771\n",
      "iteration 3000 / 4000: loss 1.343409\n",
      "iteration 3100 / 4000: loss 1.387208\n",
      "iteration 3200 / 4000: loss 1.354474\n",
      "iteration 3300 / 4000: loss 1.393109\n",
      "iteration 3400 / 4000: loss 1.362447\n",
      "iteration 3500 / 4000: loss 1.360201\n",
      "iteration 3600 / 4000: loss 1.368267\n",
      "iteration 3700 / 4000: loss 1.319770\n",
      "iteration 3800 / 4000: loss 1.297028\n",
      "iteration 3900 / 4000: loss 1.348826\n",
      "iteration 0 / 4000: loss 1.609458\n",
      "iteration 100 / 4000: loss 1.262012\n",
      "iteration 200 / 4000: loss 1.189553\n",
      "iteration 300 / 4000: loss 1.202052\n",
      "iteration 400 / 4000: loss 1.190171\n",
      "iteration 500 / 4000: loss 1.155107\n",
      "iteration 600 / 4000: loss 1.216231\n",
      "iteration 700 / 4000: loss 1.147611\n",
      "iteration 800 / 4000: loss 1.170837\n",
      "iteration 900 / 4000: loss 1.174688\n",
      "iteration 1000 / 4000: loss 1.190881\n",
      "iteration 1100 / 4000: loss 1.278316\n",
      "iteration 1200 / 4000: loss 1.186927\n",
      "iteration 1300 / 4000: loss 1.110369\n",
      "iteration 1400 / 4000: loss 1.208116\n",
      "iteration 1500 / 4000: loss 1.192556\n",
      "iteration 1600 / 4000: loss 1.191994\n",
      "iteration 1700 / 4000: loss 1.198529\n",
      "iteration 1800 / 4000: loss 1.195961\n",
      "iteration 1900 / 4000: loss 1.141655\n",
      "iteration 2000 / 4000: loss 1.154205\n",
      "iteration 2100 / 4000: loss 1.200612\n",
      "iteration 2200 / 4000: loss 1.189762\n",
      "iteration 2300 / 4000: loss 1.236024\n",
      "iteration 2400 / 4000: loss 1.167013\n",
      "iteration 2500 / 4000: loss 1.128338\n",
      "iteration 2600 / 4000: loss 1.255923\n",
      "iteration 2700 / 4000: loss 1.094525\n",
      "iteration 2800 / 4000: loss 1.150200\n",
      "iteration 2900 / 4000: loss 1.169702\n",
      "iteration 3000 / 4000: loss 1.184919\n",
      "iteration 3100 / 4000: loss 1.191179\n",
      "iteration 3200 / 4000: loss 1.192723\n",
      "iteration 3300 / 4000: loss 1.061620\n",
      "iteration 3400 / 4000: loss 1.198552\n",
      "iteration 3500 / 4000: loss 1.146926\n",
      "iteration 3600 / 4000: loss 1.165051\n",
      "iteration 3700 / 4000: loss 1.192950\n",
      "iteration 3800 / 4000: loss 1.284791\n",
      "iteration 3900 / 4000: loss 1.177117\n",
      "iteration 0 / 4000: loss 1.609138\n",
      "iteration 100 / 4000: loss 1.295878\n",
      "iteration 200 / 4000: loss 1.390724\n",
      "iteration 300 / 4000: loss 1.311100\n",
      "iteration 400 / 4000: loss 1.316185\n",
      "iteration 500 / 4000: loss 1.197637\n",
      "iteration 600 / 4000: loss 1.272158\n",
      "iteration 700 / 4000: loss 1.227943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 4000: loss 1.196317\n",
      "iteration 900 / 4000: loss 1.247164\n",
      "iteration 1000 / 4000: loss 1.240033\n",
      "iteration 1100 / 4000: loss 1.213128\n",
      "iteration 1200 / 4000: loss 1.298701\n",
      "iteration 1300 / 4000: loss 1.288392\n",
      "iteration 1400 / 4000: loss 1.259814\n",
      "iteration 1500 / 4000: loss 1.222061\n",
      "iteration 1600 / 4000: loss 1.185449\n",
      "iteration 1700 / 4000: loss 1.212591\n",
      "iteration 1800 / 4000: loss 1.216536\n",
      "iteration 1900 / 4000: loss 1.279856\n",
      "iteration 2000 / 4000: loss 1.168647\n",
      "iteration 2100 / 4000: loss 1.210583\n",
      "iteration 2200 / 4000: loss 1.203901\n",
      "iteration 2300 / 4000: loss 1.200515\n",
      "iteration 2400 / 4000: loss 1.243652\n",
      "iteration 2500 / 4000: loss 1.201535\n",
      "iteration 2600 / 4000: loss 1.266821\n",
      "iteration 2700 / 4000: loss 1.159557\n",
      "iteration 2800 / 4000: loss 1.215411\n",
      "iteration 2900 / 4000: loss 1.164414\n",
      "iteration 3000 / 4000: loss 1.254312\n",
      "iteration 3100 / 4000: loss 1.256467\n",
      "iteration 3200 / 4000: loss 1.267816\n",
      "iteration 3300 / 4000: loss 1.143886\n",
      "iteration 3400 / 4000: loss 1.189507\n",
      "iteration 3500 / 4000: loss 1.228518\n",
      "iteration 3600 / 4000: loss 1.143592\n",
      "iteration 3700 / 4000: loss 1.247626\n",
      "iteration 3800 / 4000: loss 1.197198\n",
      "iteration 3900 / 4000: loss 1.196769\n",
      "iteration 0 / 4000: loss 1.609146\n",
      "iteration 100 / 4000: loss 1.484018\n",
      "iteration 200 / 4000: loss 1.402217\n",
      "iteration 300 / 4000: loss 1.374078\n",
      "iteration 400 / 4000: loss 1.334135\n",
      "iteration 500 / 4000: loss 1.325897\n",
      "iteration 600 / 4000: loss 1.325390\n",
      "iteration 700 / 4000: loss 1.336536\n",
      "iteration 800 / 4000: loss 1.311873\n",
      "iteration 900 / 4000: loss 1.273016\n",
      "iteration 1000 / 4000: loss 1.301265\n",
      "iteration 1100 / 4000: loss 1.316361\n",
      "iteration 1200 / 4000: loss 1.292840\n",
      "iteration 1300 / 4000: loss 1.341458\n",
      "iteration 1400 / 4000: loss 1.258253\n",
      "iteration 1500 / 4000: loss 1.289651\n",
      "iteration 1600 / 4000: loss 1.342047\n",
      "iteration 1700 / 4000: loss 1.267178\n",
      "iteration 1800 / 4000: loss 1.224568\n",
      "iteration 1900 / 4000: loss 1.268592\n",
      "iteration 2000 / 4000: loss 1.303371\n",
      "iteration 2100 / 4000: loss 1.286806\n",
      "iteration 2200 / 4000: loss 1.276352\n",
      "iteration 2300 / 4000: loss 1.370567\n",
      "iteration 2400 / 4000: loss 1.257374\n",
      "iteration 2500 / 4000: loss 1.204087\n",
      "iteration 2600 / 4000: loss 1.168786\n",
      "iteration 2700 / 4000: loss 1.190497\n",
      "iteration 2800 / 4000: loss 1.260318\n",
      "iteration 2900 / 4000: loss 1.260093\n",
      "iteration 3000 / 4000: loss 1.258509\n",
      "iteration 3100 / 4000: loss 1.209246\n",
      "iteration 3200 / 4000: loss 1.224164\n",
      "iteration 3300 / 4000: loss 1.250692\n",
      "iteration 3400 / 4000: loss 1.253501\n",
      "iteration 3500 / 4000: loss 1.235515\n",
      "iteration 3600 / 4000: loss 1.186536\n",
      "iteration 3700 / 4000: loss 1.202576\n",
      "iteration 3800 / 4000: loss 1.148683\n",
      "iteration 3900 / 4000: loss 1.259236\n",
      "iteration 0 / 4000: loss 1.610059\n",
      "iteration 100 / 4000: loss 1.568660\n",
      "iteration 200 / 4000: loss 1.535314\n",
      "iteration 300 / 4000: loss 1.504865\n",
      "iteration 400 / 4000: loss 1.446321\n",
      "iteration 500 / 4000: loss 1.422386\n",
      "iteration 600 / 4000: loss 1.434309\n",
      "iteration 700 / 4000: loss 1.411670\n",
      "iteration 800 / 4000: loss 1.411329\n",
      "iteration 900 / 4000: loss 1.364449\n",
      "iteration 1000 / 4000: loss 1.320902\n",
      "iteration 1100 / 4000: loss 1.385586\n",
      "iteration 1200 / 4000: loss 1.327281\n",
      "iteration 1300 / 4000: loss 1.356217\n",
      "iteration 1400 / 4000: loss 1.280756\n",
      "iteration 1500 / 4000: loss 1.342093\n",
      "iteration 1600 / 4000: loss 1.286520\n",
      "iteration 1700 / 4000: loss 1.349490\n",
      "iteration 1800 / 4000: loss 1.312230\n",
      "iteration 1900 / 4000: loss 1.283913\n",
      "iteration 2000 / 4000: loss 1.332392\n",
      "iteration 2100 / 4000: loss 1.270613\n",
      "iteration 2200 / 4000: loss 1.286369\n",
      "iteration 2300 / 4000: loss 1.246905\n",
      "iteration 2400 / 4000: loss 1.301876\n",
      "iteration 2500 / 4000: loss 1.273943\n",
      "iteration 2600 / 4000: loss 1.284739\n",
      "iteration 2700 / 4000: loss 1.299043\n",
      "iteration 2800 / 4000: loss 1.246671\n",
      "iteration 2900 / 4000: loss 1.245964\n",
      "iteration 3000 / 4000: loss 1.329757\n",
      "iteration 3100 / 4000: loss 1.263504\n",
      "iteration 3200 / 4000: loss 1.328895\n",
      "iteration 3300 / 4000: loss 1.345847\n",
      "iteration 3400 / 4000: loss 1.291412\n",
      "iteration 3500 / 4000: loss 1.275825\n",
      "iteration 3600 / 4000: loss 1.326671\n",
      "iteration 3700 / 4000: loss 1.276057\n",
      "iteration 3800 / 4000: loss 1.229766\n",
      "iteration 3900 / 4000: loss 1.213883\n",
      "iteration 0 / 4000: loss 1.608706\n",
      "iteration 100 / 4000: loss 1.594020\n",
      "iteration 200 / 4000: loss 1.580485\n",
      "iteration 300 / 4000: loss 1.564485\n",
      "iteration 400 / 4000: loss 1.556079\n",
      "iteration 500 / 4000: loss 1.546132\n",
      "iteration 600 / 4000: loss 1.520969\n",
      "iteration 700 / 4000: loss 1.523695\n",
      "iteration 800 / 4000: loss 1.531342\n",
      "iteration 900 / 4000: loss 1.488295\n",
      "iteration 1000 / 4000: loss 1.475151\n",
      "iteration 1100 / 4000: loss 1.484269\n",
      "iteration 1200 / 4000: loss 1.463808\n",
      "iteration 1300 / 4000: loss 1.461780\n",
      "iteration 1400 / 4000: loss 1.451299\n",
      "iteration 1500 / 4000: loss 1.430949\n",
      "iteration 1600 / 4000: loss 1.434603\n",
      "iteration 1700 / 4000: loss 1.456835\n",
      "iteration 1800 / 4000: loss 1.416032\n",
      "iteration 1900 / 4000: loss 1.422978\n",
      "iteration 2000 / 4000: loss 1.396758\n",
      "iteration 2100 / 4000: loss 1.419346\n",
      "iteration 2200 / 4000: loss 1.403188\n",
      "iteration 2300 / 4000: loss 1.406838\n",
      "iteration 2400 / 4000: loss 1.408507\n",
      "iteration 2500 / 4000: loss 1.458894\n",
      "iteration 2600 / 4000: loss 1.365621\n",
      "iteration 2700 / 4000: loss 1.344287\n",
      "iteration 2800 / 4000: loss 1.346408\n",
      "iteration 2900 / 4000: loss 1.358451\n",
      "iteration 3000 / 4000: loss 1.375434\n",
      "iteration 3100 / 4000: loss 1.368499\n",
      "iteration 3200 / 4000: loss 1.329044\n",
      "iteration 3300 / 4000: loss 1.375097\n",
      "iteration 3400 / 4000: loss 1.345951\n",
      "iteration 3500 / 4000: loss 1.331142\n",
      "iteration 3600 / 4000: loss 1.336628\n",
      "iteration 3700 / 4000: loss 1.324493\n",
      "iteration 3800 / 4000: loss 1.295434\n",
      "iteration 3900 / 4000: loss 1.336841\n",
      "iteration 0 / 4000: loss 1.608727\n",
      "iteration 100 / 4000: loss 1.322270\n",
      "iteration 200 / 4000: loss 1.228203\n",
      "iteration 300 / 4000: loss 1.342472\n",
      "iteration 400 / 4000: loss 1.216707\n",
      "iteration 500 / 4000: loss 1.246611\n",
      "iteration 600 / 4000: loss 1.222877\n",
      "iteration 700 / 4000: loss 1.207110\n",
      "iteration 800 / 4000: loss 1.231746\n",
      "iteration 900 / 4000: loss 1.209169\n",
      "iteration 1000 / 4000: loss 1.282205\n",
      "iteration 1100 / 4000: loss 1.080158\n",
      "iteration 1200 / 4000: loss 1.177324\n",
      "iteration 1300 / 4000: loss 1.372930\n",
      "iteration 1400 / 4000: loss 1.207882\n",
      "iteration 1500 / 4000: loss 1.194135\n",
      "iteration 1600 / 4000: loss 1.172226\n",
      "iteration 1700 / 4000: loss 1.189444\n",
      "iteration 1800 / 4000: loss 1.119450\n",
      "iteration 1900 / 4000: loss 1.157831\n",
      "iteration 2000 / 4000: loss 1.234365\n",
      "iteration 2100 / 4000: loss 1.068463\n",
      "iteration 2200 / 4000: loss 1.140593\n",
      "iteration 2300 / 4000: loss 1.282147\n",
      "iteration 2400 / 4000: loss 1.079948\n",
      "iteration 2500 / 4000: loss 1.213291\n",
      "iteration 2600 / 4000: loss 1.210826\n",
      "iteration 2700 / 4000: loss 1.234738\n",
      "iteration 2800 / 4000: loss 1.178112\n",
      "iteration 2900 / 4000: loss 1.200239\n",
      "iteration 3000 / 4000: loss 1.100724\n",
      "iteration 3100 / 4000: loss 1.163939\n",
      "iteration 3200 / 4000: loss 1.095115\n",
      "iteration 3300 / 4000: loss 1.206804\n",
      "iteration 3400 / 4000: loss 1.248388\n",
      "iteration 3500 / 4000: loss 1.129534\n",
      "iteration 3600 / 4000: loss 1.239147\n",
      "iteration 3700 / 4000: loss 1.133372\n",
      "iteration 3800 / 4000: loss 1.055680\n",
      "iteration 3900 / 4000: loss 1.140944\n",
      "iteration 0 / 4000: loss 1.610087\n",
      "iteration 100 / 4000: loss 1.347495\n",
      "iteration 200 / 4000: loss 1.338183\n",
      "iteration 300 / 4000: loss 1.315012\n",
      "iteration 400 / 4000: loss 1.291838\n",
      "iteration 500 / 4000: loss 1.215612\n",
      "iteration 600 / 4000: loss 1.269364\n",
      "iteration 700 / 4000: loss 1.238383\n",
      "iteration 800 / 4000: loss 1.174583\n",
      "iteration 900 / 4000: loss 1.161025\n",
      "iteration 1000 / 4000: loss 1.223213\n",
      "iteration 1100 / 4000: loss 1.209196\n",
      "iteration 1200 / 4000: loss 1.186327\n",
      "iteration 1300 / 4000: loss 1.243611\n",
      "iteration 1400 / 4000: loss 1.115522\n",
      "iteration 1500 / 4000: loss 1.238621\n",
      "iteration 1600 / 4000: loss 1.228313\n",
      "iteration 1700 / 4000: loss 1.187875\n",
      "iteration 1800 / 4000: loss 1.171433\n",
      "iteration 1900 / 4000: loss 1.213315\n",
      "iteration 2000 / 4000: loss 1.260165\n",
      "iteration 2100 / 4000: loss 1.257597\n",
      "iteration 2200 / 4000: loss 1.216284\n",
      "iteration 2300 / 4000: loss 1.147505\n",
      "iteration 2400 / 4000: loss 1.228446\n",
      "iteration 2500 / 4000: loss 1.141225\n",
      "iteration 2600 / 4000: loss 1.244700\n",
      "iteration 2700 / 4000: loss 1.167124\n",
      "iteration 2800 / 4000: loss 1.353306\n",
      "iteration 2900 / 4000: loss 1.145415\n",
      "iteration 3000 / 4000: loss 1.235272\n",
      "iteration 3100 / 4000: loss 1.227166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200 / 4000: loss 1.203861\n",
      "iteration 3300 / 4000: loss 1.185988\n",
      "iteration 3400 / 4000: loss 1.158934\n",
      "iteration 3500 / 4000: loss 1.237348\n",
      "iteration 3600 / 4000: loss 1.159091\n",
      "iteration 3700 / 4000: loss 1.182445\n",
      "iteration 3800 / 4000: loss 1.204650\n",
      "iteration 3900 / 4000: loss 1.169337\n",
      "iteration 0 / 4000: loss 1.609801\n",
      "iteration 100 / 4000: loss 1.490317\n",
      "iteration 200 / 4000: loss 1.401683\n",
      "iteration 300 / 4000: loss 1.405342\n",
      "iteration 400 / 4000: loss 1.347737\n",
      "iteration 500 / 4000: loss 1.334167\n",
      "iteration 600 / 4000: loss 1.367468\n",
      "iteration 700 / 4000: loss 1.267551\n",
      "iteration 800 / 4000: loss 1.263371\n",
      "iteration 900 / 4000: loss 1.205979\n",
      "iteration 1000 / 4000: loss 1.340742\n",
      "iteration 1100 / 4000: loss 1.231666\n",
      "iteration 1200 / 4000: loss 1.265848\n",
      "iteration 1300 / 4000: loss 1.248696\n",
      "iteration 1400 / 4000: loss 1.325587\n",
      "iteration 1500 / 4000: loss 1.233899\n",
      "iteration 1600 / 4000: loss 1.272338\n",
      "iteration 1700 / 4000: loss 1.211354\n",
      "iteration 1800 / 4000: loss 1.260487\n",
      "iteration 1900 / 4000: loss 1.212809\n",
      "iteration 2000 / 4000: loss 1.247804\n",
      "iteration 2100 / 4000: loss 1.099783\n",
      "iteration 2200 / 4000: loss 1.183300\n",
      "iteration 2300 / 4000: loss 1.265432\n",
      "iteration 2400 / 4000: loss 1.244116\n",
      "iteration 2500 / 4000: loss 1.213408\n",
      "iteration 2600 / 4000: loss 1.190128\n",
      "iteration 2700 / 4000: loss 1.260674\n",
      "iteration 2800 / 4000: loss 1.261576\n",
      "iteration 2900 / 4000: loss 1.197881\n",
      "iteration 3000 / 4000: loss 1.206781\n",
      "iteration 3100 / 4000: loss 1.251803\n",
      "iteration 3200 / 4000: loss 1.281962\n",
      "iteration 3300 / 4000: loss 1.234363\n",
      "iteration 3400 / 4000: loss 1.233394\n",
      "iteration 3500 / 4000: loss 1.258067\n",
      "iteration 3600 / 4000: loss 1.130111\n",
      "iteration 3700 / 4000: loss 1.264882\n",
      "iteration 3800 / 4000: loss 1.259650\n",
      "iteration 3900 / 4000: loss 1.285027\n",
      "iteration 0 / 4000: loss 1.609787\n",
      "iteration 100 / 4000: loss 1.562232\n",
      "iteration 200 / 4000: loss 1.519516\n",
      "iteration 300 / 4000: loss 1.485474\n",
      "iteration 400 / 4000: loss 1.461536\n",
      "iteration 500 / 4000: loss 1.414909\n",
      "iteration 600 / 4000: loss 1.384301\n",
      "iteration 700 / 4000: loss 1.415358\n",
      "iteration 800 / 4000: loss 1.409653\n",
      "iteration 900 / 4000: loss 1.378340\n",
      "iteration 1000 / 4000: loss 1.349284\n",
      "iteration 1100 / 4000: loss 1.325496\n",
      "iteration 1200 / 4000: loss 1.333534\n",
      "iteration 1300 / 4000: loss 1.382013\n",
      "iteration 1400 / 4000: loss 1.275369\n",
      "iteration 1500 / 4000: loss 1.322456\n",
      "iteration 1600 / 4000: loss 1.324317\n",
      "iteration 1700 / 4000: loss 1.358357\n",
      "iteration 1800 / 4000: loss 1.332872\n",
      "iteration 1900 / 4000: loss 1.264431\n",
      "iteration 2000 / 4000: loss 1.315185\n",
      "iteration 2100 / 4000: loss 1.316225\n",
      "iteration 2200 / 4000: loss 1.375722\n",
      "iteration 2300 / 4000: loss 1.318270\n",
      "iteration 2400 / 4000: loss 1.246320\n",
      "iteration 2500 / 4000: loss 1.304225\n",
      "iteration 2600 / 4000: loss 1.225253\n",
      "iteration 2700 / 4000: loss 1.278961\n",
      "iteration 2800 / 4000: loss 1.316633\n",
      "iteration 2900 / 4000: loss 1.292427\n",
      "iteration 3000 / 4000: loss 1.275066\n",
      "iteration 3100 / 4000: loss 1.273336\n",
      "iteration 3200 / 4000: loss 1.198564\n",
      "iteration 3300 / 4000: loss 1.298408\n",
      "iteration 3400 / 4000: loss 1.291954\n",
      "iteration 3500 / 4000: loss 1.370195\n",
      "iteration 3600 / 4000: loss 1.267856\n",
      "iteration 3700 / 4000: loss 1.298259\n",
      "iteration 3800 / 4000: loss 1.226611\n",
      "iteration 3900 / 4000: loss 1.206174\n",
      "iteration 0 / 4000: loss 1.609226\n",
      "iteration 100 / 4000: loss 1.593168\n",
      "iteration 200 / 4000: loss 1.574632\n",
      "iteration 300 / 4000: loss 1.566233\n",
      "iteration 400 / 4000: loss 1.565090\n",
      "iteration 500 / 4000: loss 1.539861\n",
      "iteration 600 / 4000: loss 1.519938\n",
      "iteration 700 / 4000: loss 1.513174\n",
      "iteration 800 / 4000: loss 1.496548\n",
      "iteration 900 / 4000: loss 1.485962\n",
      "iteration 1000 / 4000: loss 1.485215\n",
      "iteration 1100 / 4000: loss 1.461006\n",
      "iteration 1200 / 4000: loss 1.490783\n",
      "iteration 1300 / 4000: loss 1.436062\n",
      "iteration 1400 / 4000: loss 1.434597\n",
      "iteration 1500 / 4000: loss 1.448576\n",
      "iteration 1600 / 4000: loss 1.461640\n",
      "iteration 1700 / 4000: loss 1.403011\n",
      "iteration 1800 / 4000: loss 1.382560\n",
      "iteration 1900 / 4000: loss 1.408732\n",
      "iteration 2000 / 4000: loss 1.373237\n",
      "iteration 2100 / 4000: loss 1.364147\n",
      "iteration 2200 / 4000: loss 1.422940\n",
      "iteration 2300 / 4000: loss 1.389347\n",
      "iteration 2400 / 4000: loss 1.420355\n",
      "iteration 2500 / 4000: loss 1.396715\n",
      "iteration 2600 / 4000: loss 1.404376\n",
      "iteration 2700 / 4000: loss 1.345752\n",
      "iteration 2800 / 4000: loss 1.404637\n",
      "iteration 2900 / 4000: loss 1.353006\n",
      "iteration 3000 / 4000: loss 1.370724\n",
      "iteration 3100 / 4000: loss 1.367979\n",
      "iteration 3200 / 4000: loss 1.376298\n",
      "iteration 3300 / 4000: loss 1.356423\n",
      "iteration 3400 / 4000: loss 1.340907\n",
      "iteration 3500 / 4000: loss 1.367164\n",
      "iteration 3600 / 4000: loss 1.382949\n",
      "iteration 3700 / 4000: loss 1.312693\n",
      "iteration 3800 / 4000: loss 1.376375\n",
      "iteration 3900 / 4000: loss 1.319852\n"
     ]
    }
   ],
   "source": [
    "from softmax import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3]\n",
    "features = [0,1,2,3]\n",
    "\n",
    "\n",
    "for i in features:\n",
    "    X_train = np.copy(train_text)\n",
    "    X_val = np.copy(val_text)\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else: \n",
    "        X_train = np.delete(train_text,list(range(1000*(i-1),1000*i)),axis = 1)\n",
    "        X_val = np.delete(val_text,list(range(1000*(i-1),1000*i)), axis = 1)\n",
    "        \n",
    "    #添加bias\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    for lr in learning_rates:\n",
    "        model = Softmax()\n",
    "        model.train(X_train, y_train, learning_rate = lr, num_iters = 4000)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        acc_train = np.mean(y_pred_train == y_train)\n",
    "        \n",
    "        y_pred_val = model.predict(X_val)\n",
    "        acc_val = np.mean(y_pred_val == y_val)\n",
    "        results[(lr,i)] = (acc_train, acc_val)\n",
    "        \n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_softmax = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during validation: 0.546887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "softmax_df = pd.DataFrame(np.zeros((4,5)),index = ['N1,N2,N3','N2,N3','N1,N3','N1,N2'],columns = learning_rates)\n",
    "feature_dict = {\n",
    "    0:'N1,N2,N3',\n",
    "    1:'N2,N3',\n",
    "    2:'N1,N3',\n",
    "    3:'N1,N2'\n",
    "}\n",
    "for lr, gram in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, gram)]\n",
    "    softmax_df[lr][feature_dict[gram]] = val_accuracy\n",
    "    \n",
    "print('best validation accuracy achieved during validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.03</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.003</th>\n",
       "      <th>0.001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N1,N2,N3</th>\n",
       "      <td>0.546887</td>\n",
       "      <td>0.532690</td>\n",
       "      <td>0.522354</td>\n",
       "      <td>0.511395</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N2,N3</th>\n",
       "      <td>0.519738</td>\n",
       "      <td>0.512267</td>\n",
       "      <td>0.510212</td>\n",
       "      <td>0.510212</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1,N3</th>\n",
       "      <td>0.545890</td>\n",
       "      <td>0.530262</td>\n",
       "      <td>0.520486</td>\n",
       "      <td>0.511083</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1,N2</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.532379</td>\n",
       "      <td>0.522790</td>\n",
       "      <td>0.511208</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0.100     0.030     0.010     0.003     0.001\n",
       "N1,N2,N3  0.546887  0.532690  0.522354  0.511395  0.510212\n",
       "N2,N3     0.519738  0.512267  0.510212  0.510212  0.510212\n",
       "N1,N3     0.545890  0.530262  0.520486  0.511083  0.510212\n",
       "N1,N2     0.545455  0.532379  0.522790  0.511208  0.510212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.copy(test_text)\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "test_predictions = best_softmax.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    'PhraseId': test['PhraseId'],\n",
    "    'Sentiment': test_predictions\n",
    "})\n",
    "\n",
    "output.to_csv('softmax_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上传至kaggle后，得分为0.55050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(hinge loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 4000: loss 4.003538\n",
      "iteration 100 / 4000: loss 2.111532\n",
      "iteration 200 / 4000: loss 1.938536\n",
      "iteration 300 / 4000: loss 1.876652\n",
      "iteration 400 / 4000: loss 1.649497\n",
      "iteration 500 / 4000: loss 1.846131\n",
      "iteration 600 / 4000: loss 1.688485\n",
      "iteration 700 / 4000: loss 1.648000\n",
      "iteration 800 / 4000: loss 1.756324\n",
      "iteration 900 / 4000: loss 1.853997\n",
      "iteration 1000 / 4000: loss 1.789751\n",
      "iteration 1100 / 4000: loss 1.668364\n",
      "iteration 1200 / 4000: loss 1.586392\n",
      "iteration 1300 / 4000: loss 1.892216\n",
      "iteration 1400 / 4000: loss 1.684077\n",
      "iteration 1500 / 4000: loss 1.649654\n",
      "iteration 1600 / 4000: loss 1.729871\n",
      "iteration 1700 / 4000: loss 1.459974\n",
      "iteration 1800 / 4000: loss 1.643731\n",
      "iteration 1900 / 4000: loss 1.866728\n",
      "iteration 2000 / 4000: loss 1.686098\n",
      "iteration 2100 / 4000: loss 1.659622\n",
      "iteration 2200 / 4000: loss 1.632813\n",
      "iteration 2300 / 4000: loss 1.791769\n",
      "iteration 2400 / 4000: loss 1.708221\n",
      "iteration 2500 / 4000: loss 1.448448\n",
      "iteration 2600 / 4000: loss 1.493874\n",
      "iteration 2700 / 4000: loss 1.775634\n",
      "iteration 2800 / 4000: loss 1.675162\n",
      "iteration 2900 / 4000: loss 1.768155\n",
      "iteration 3000 / 4000: loss 1.584077\n",
      "iteration 3100 / 4000: loss 1.764144\n",
      "iteration 3200 / 4000: loss 1.555354\n",
      "iteration 3300 / 4000: loss 1.832879\n",
      "iteration 3400 / 4000: loss 1.608889\n",
      "iteration 3500 / 4000: loss 1.731360\n",
      "iteration 3600 / 4000: loss 1.423215\n",
      "iteration 3700 / 4000: loss 1.595655\n",
      "iteration 3800 / 4000: loss 1.476180\n",
      "iteration 3900 / 4000: loss 1.691436\n",
      "iteration 0 / 4000: loss 4.002716\n",
      "iteration 100 / 4000: loss 1.891601\n",
      "iteration 200 / 4000: loss 2.140877\n",
      "iteration 300 / 4000: loss 1.760738\n",
      "iteration 400 / 4000: loss 1.952196\n",
      "iteration 500 / 4000: loss 1.693005\n",
      "iteration 600 / 4000: loss 1.719643\n",
      "iteration 700 / 4000: loss 1.675616\n",
      "iteration 800 / 4000: loss 1.927647\n",
      "iteration 900 / 4000: loss 2.029076\n",
      "iteration 1000 / 4000: loss 1.820371\n",
      "iteration 1100 / 4000: loss 1.959475\n",
      "iteration 1200 / 4000: loss 1.937374\n",
      "iteration 1300 / 4000: loss 1.803750\n",
      "iteration 1400 / 4000: loss 1.551471\n",
      "iteration 1500 / 4000: loss 2.144669\n",
      "iteration 1600 / 4000: loss 1.895238\n",
      "iteration 1700 / 4000: loss 1.950983\n",
      "iteration 1800 / 4000: loss 1.713619\n",
      "iteration 1900 / 4000: loss 2.097619\n",
      "iteration 2000 / 4000: loss 1.600241\n",
      "iteration 2100 / 4000: loss 1.726022\n",
      "iteration 2200 / 4000: loss 1.866985\n",
      "iteration 2300 / 4000: loss 2.068084\n",
      "iteration 2400 / 4000: loss 1.809962\n",
      "iteration 2500 / 4000: loss 1.659990\n",
      "iteration 2600 / 4000: loss 1.910856\n",
      "iteration 2700 / 4000: loss 1.806309\n",
      "iteration 2800 / 4000: loss 1.732858\n",
      "iteration 2900 / 4000: loss 1.921034\n",
      "iteration 3000 / 4000: loss 1.815065\n",
      "iteration 3100 / 4000: loss 1.856271\n",
      "iteration 3200 / 4000: loss 1.685069\n",
      "iteration 3300 / 4000: loss 1.861934\n",
      "iteration 3400 / 4000: loss 1.847790\n",
      "iteration 3500 / 4000: loss 1.869720\n",
      "iteration 3600 / 4000: loss 1.464443\n",
      "iteration 3700 / 4000: loss 1.734769\n",
      "iteration 3800 / 4000: loss 1.789485\n",
      "iteration 3900 / 4000: loss 1.878462\n",
      "iteration 0 / 4000: loss 4.000641\n",
      "iteration 100 / 4000: loss 2.221914\n",
      "iteration 200 / 4000: loss 2.199887\n",
      "iteration 300 / 4000: loss 1.822751\n",
      "iteration 400 / 4000: loss 1.789013\n",
      "iteration 500 / 4000: loss 2.115278\n",
      "iteration 600 / 4000: loss 2.014651\n",
      "iteration 700 / 4000: loss 2.099950\n",
      "iteration 800 / 4000: loss 2.129599\n",
      "iteration 900 / 4000: loss 1.746104\n",
      "iteration 1000 / 4000: loss 1.596177\n",
      "iteration 1100 / 4000: loss 1.719485\n",
      "iteration 1200 / 4000: loss 1.821671\n",
      "iteration 1300 / 4000: loss 1.723640\n",
      "iteration 1400 / 4000: loss 1.963274\n",
      "iteration 1500 / 4000: loss 1.858207\n",
      "iteration 1600 / 4000: loss 2.162276\n",
      "iteration 1700 / 4000: loss 1.857515\n",
      "iteration 1800 / 4000: loss 1.715069\n",
      "iteration 1900 / 4000: loss 1.949240\n",
      "iteration 2000 / 4000: loss 2.207492\n",
      "iteration 2100 / 4000: loss 1.887101\n",
      "iteration 2200 / 4000: loss 1.804096\n",
      "iteration 2300 / 4000: loss 1.977747\n",
      "iteration 2400 / 4000: loss 1.963981\n",
      "iteration 2500 / 4000: loss 1.697331\n",
      "iteration 2600 / 4000: loss 1.701373\n",
      "iteration 2700 / 4000: loss 1.893011\n",
      "iteration 2800 / 4000: loss 2.081852\n",
      "iteration 2900 / 4000: loss 1.930678\n",
      "iteration 3000 / 4000: loss 1.707192\n",
      "iteration 3100 / 4000: loss 1.885399\n",
      "iteration 3200 / 4000: loss 1.936927\n",
      "iteration 3300 / 4000: loss 1.842021\n",
      "iteration 3400 / 4000: loss 1.986135\n",
      "iteration 3500 / 4000: loss 1.978136\n",
      "iteration 3600 / 4000: loss 1.904660\n",
      "iteration 3700 / 4000: loss 1.902707\n",
      "iteration 3800 / 4000: loss 2.129439\n",
      "iteration 3900 / 4000: loss 1.791614\n",
      "iteration 0 / 4000: loss 3.998649\n",
      "iteration 100 / 4000: loss 2.687410\n",
      "iteration 200 / 4000: loss 2.200823\n",
      "iteration 300 / 4000: loss 2.393901\n",
      "iteration 400 / 4000: loss 1.991416\n",
      "iteration 500 / 4000: loss 2.139856\n",
      "iteration 600 / 4000: loss 2.096908\n",
      "iteration 700 / 4000: loss 2.005591\n",
      "iteration 800 / 4000: loss 1.908043\n",
      "iteration 900 / 4000: loss 1.622216\n",
      "iteration 1000 / 4000: loss 2.370801\n",
      "iteration 1100 / 4000: loss 1.889319\n",
      "iteration 1200 / 4000: loss 1.818006\n",
      "iteration 1300 / 4000: loss 2.134435\n",
      "iteration 1400 / 4000: loss 2.062808\n",
      "iteration 1500 / 4000: loss 1.716096\n",
      "iteration 1600 / 4000: loss 2.004602\n",
      "iteration 1700 / 4000: loss 2.248758\n",
      "iteration 1800 / 4000: loss 1.941272\n",
      "iteration 1900 / 4000: loss 1.830507\n",
      "iteration 2000 / 4000: loss 2.079020\n",
      "iteration 2100 / 4000: loss 2.063246\n",
      "iteration 2200 / 4000: loss 1.833707\n",
      "iteration 2300 / 4000: loss 2.105465\n",
      "iteration 2400 / 4000: loss 1.643741\n",
      "iteration 2500 / 4000: loss 2.182100\n",
      "iteration 2600 / 4000: loss 1.839316\n",
      "iteration 2700 / 4000: loss 1.962886\n",
      "iteration 2800 / 4000: loss 1.847115\n",
      "iteration 2900 / 4000: loss 2.135747\n",
      "iteration 3000 / 4000: loss 1.787553\n",
      "iteration 3100 / 4000: loss 1.867107\n",
      "iteration 3200 / 4000: loss 2.180220\n",
      "iteration 3300 / 4000: loss 2.171541\n",
      "iteration 3400 / 4000: loss 1.889946\n",
      "iteration 3500 / 4000: loss 2.112186\n",
      "iteration 3600 / 4000: loss 1.887676\n",
      "iteration 3700 / 4000: loss 1.787477\n",
      "iteration 3800 / 4000: loss 1.962805\n",
      "iteration 3900 / 4000: loss 1.724892\n",
      "iteration 0 / 4000: loss 3.996233\n",
      "iteration 100 / 4000: loss 3.637263\n",
      "iteration 200 / 4000: loss 3.242327\n",
      "iteration 300 / 4000: loss 2.835194\n",
      "iteration 400 / 4000: loss 2.533029\n",
      "iteration 500 / 4000: loss 2.387894\n",
      "iteration 600 / 4000: loss 2.170579\n",
      "iteration 700 / 4000: loss 2.243099\n",
      "iteration 800 / 4000: loss 2.198003\n",
      "iteration 900 / 4000: loss 2.368942\n",
      "iteration 1000 / 4000: loss 1.848392\n",
      "iteration 1100 / 4000: loss 2.229770\n",
      "iteration 1200 / 4000: loss 2.178988\n",
      "iteration 1300 / 4000: loss 2.098442\n",
      "iteration 1400 / 4000: loss 2.438271\n",
      "iteration 1500 / 4000: loss 1.944526\n",
      "iteration 1600 / 4000: loss 1.760681\n",
      "iteration 1700 / 4000: loss 2.160991\n",
      "iteration 1800 / 4000: loss 1.892652\n",
      "iteration 1900 / 4000: loss 2.014942\n",
      "iteration 2000 / 4000: loss 1.926581\n",
      "iteration 2100 / 4000: loss 2.125281\n",
      "iteration 2200 / 4000: loss 2.068953\n",
      "iteration 2300 / 4000: loss 2.118365\n",
      "iteration 2400 / 4000: loss 1.881107\n",
      "iteration 2500 / 4000: loss 1.924445\n",
      "iteration 2600 / 4000: loss 2.498090\n",
      "iteration 2700 / 4000: loss 1.992821\n",
      "iteration 2800 / 4000: loss 2.238466\n",
      "iteration 2900 / 4000: loss 2.121331\n",
      "iteration 3000 / 4000: loss 2.237724\n",
      "iteration 3100 / 4000: loss 2.082711\n",
      "iteration 3200 / 4000: loss 1.774669\n",
      "iteration 3300 / 4000: loss 2.216744\n",
      "iteration 3400 / 4000: loss 2.113145\n",
      "iteration 3500 / 4000: loss 2.320459\n",
      "iteration 3600 / 4000: loss 1.921040\n",
      "iteration 3700 / 4000: loss 1.953937\n",
      "iteration 3800 / 4000: loss 2.043729\n",
      "iteration 3900 / 4000: loss 1.806999\n",
      "iteration 0 / 4000: loss 3.999101\n",
      "iteration 100 / 4000: loss 2.115291\n",
      "iteration 200 / 4000: loss 2.351828\n",
      "iteration 300 / 4000: loss 1.819681\n",
      "iteration 400 / 4000: loss 1.972553\n",
      "iteration 500 / 4000: loss 2.319878\n",
      "iteration 600 / 4000: loss 1.876728\n",
      "iteration 700 / 4000: loss 1.992181\n",
      "iteration 800 / 4000: loss 2.284151\n",
      "iteration 900 / 4000: loss 1.926953\n",
      "iteration 1000 / 4000: loss 2.133857\n",
      "iteration 1100 / 4000: loss 2.193475\n",
      "iteration 1200 / 4000: loss 2.209484\n",
      "iteration 1300 / 4000: loss 1.825060\n",
      "iteration 1400 / 4000: loss 1.985593\n",
      "iteration 1500 / 4000: loss 1.881595\n",
      "iteration 1600 / 4000: loss 1.750682\n",
      "iteration 1700 / 4000: loss 2.167919\n",
      "iteration 1800 / 4000: loss 1.961577\n",
      "iteration 1900 / 4000: loss 1.680395\n",
      "iteration 2000 / 4000: loss 1.975250\n",
      "iteration 2100 / 4000: loss 2.025102\n",
      "iteration 2200 / 4000: loss 1.847792\n",
      "iteration 2300 / 4000: loss 1.838826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2400 / 4000: loss 1.859196\n",
      "iteration 2500 / 4000: loss 2.215275\n",
      "iteration 2600 / 4000: loss 2.077889\n",
      "iteration 2700 / 4000: loss 2.003380\n",
      "iteration 2800 / 4000: loss 1.917269\n",
      "iteration 2900 / 4000: loss 1.984747\n",
      "iteration 3000 / 4000: loss 1.592428\n",
      "iteration 3100 / 4000: loss 1.927647\n",
      "iteration 3200 / 4000: loss 1.942808\n",
      "iteration 3300 / 4000: loss 1.910688\n",
      "iteration 3400 / 4000: loss 1.893014\n",
      "iteration 3500 / 4000: loss 1.630606\n",
      "iteration 3600 / 4000: loss 1.614956\n",
      "iteration 3700 / 4000: loss 2.196311\n",
      "iteration 3800 / 4000: loss 2.169881\n",
      "iteration 3900 / 4000: loss 1.853733\n",
      "iteration 0 / 4000: loss 3.999825\n",
      "iteration 100 / 4000: loss 1.770160\n",
      "iteration 200 / 4000: loss 2.045438\n",
      "iteration 300 / 4000: loss 1.696206\n",
      "iteration 400 / 4000: loss 1.991379\n",
      "iteration 500 / 4000: loss 1.854892\n",
      "iteration 600 / 4000: loss 2.017542\n",
      "iteration 700 / 4000: loss 2.250896\n",
      "iteration 800 / 4000: loss 2.081184\n",
      "iteration 900 / 4000: loss 1.908149\n",
      "iteration 1000 / 4000: loss 1.976040\n",
      "iteration 1100 / 4000: loss 1.796024\n",
      "iteration 1200 / 4000: loss 2.119778\n",
      "iteration 1300 / 4000: loss 2.134880\n",
      "iteration 1400 / 4000: loss 1.700159\n",
      "iteration 1500 / 4000: loss 1.879235\n",
      "iteration 1600 / 4000: loss 2.025260\n",
      "iteration 1700 / 4000: loss 1.999498\n",
      "iteration 1800 / 4000: loss 1.700554\n",
      "iteration 1900 / 4000: loss 2.035561\n",
      "iteration 2000 / 4000: loss 1.448120\n",
      "iteration 2100 / 4000: loss 1.828711\n",
      "iteration 2200 / 4000: loss 2.317619\n",
      "iteration 2300 / 4000: loss 1.853203\n",
      "iteration 2400 / 4000: loss 1.935655\n",
      "iteration 2500 / 4000: loss 1.900056\n",
      "iteration 2600 / 4000: loss 2.056514\n",
      "iteration 2700 / 4000: loss 1.855337\n",
      "iteration 2800 / 4000: loss 1.645594\n",
      "iteration 2900 / 4000: loss 2.117955\n",
      "iteration 3000 / 4000: loss 2.102370\n",
      "iteration 3100 / 4000: loss 1.903153\n",
      "iteration 3200 / 4000: loss 1.905196\n",
      "iteration 3300 / 4000: loss 1.902238\n",
      "iteration 3400 / 4000: loss 2.038196\n",
      "iteration 3500 / 4000: loss 1.837029\n",
      "iteration 3600 / 4000: loss 1.990392\n",
      "iteration 3700 / 4000: loss 1.929247\n",
      "iteration 3800 / 4000: loss 1.850089\n",
      "iteration 3900 / 4000: loss 2.273457\n",
      "iteration 0 / 4000: loss 3.997561\n",
      "iteration 100 / 4000: loss 2.156445\n",
      "iteration 200 / 4000: loss 2.108122\n",
      "iteration 300 / 4000: loss 2.314969\n",
      "iteration 400 / 4000: loss 1.855680\n",
      "iteration 500 / 4000: loss 1.899218\n",
      "iteration 600 / 4000: loss 2.081431\n",
      "iteration 700 / 4000: loss 1.946915\n",
      "iteration 800 / 4000: loss 1.887915\n",
      "iteration 900 / 4000: loss 1.901071\n",
      "iteration 1000 / 4000: loss 2.022347\n",
      "iteration 1100 / 4000: loss 2.200241\n",
      "iteration 1200 / 4000: loss 2.061890\n",
      "iteration 1300 / 4000: loss 1.906088\n",
      "iteration 1400 / 4000: loss 1.689178\n",
      "iteration 1500 / 4000: loss 2.060520\n",
      "iteration 1600 / 4000: loss 1.706559\n",
      "iteration 1700 / 4000: loss 2.019599\n",
      "iteration 1800 / 4000: loss 2.187299\n",
      "iteration 1900 / 4000: loss 2.399538\n",
      "iteration 2000 / 4000: loss 1.747216\n",
      "iteration 2100 / 4000: loss 1.751282\n",
      "iteration 2200 / 4000: loss 1.867665\n",
      "iteration 2300 / 4000: loss 1.787862\n",
      "iteration 2400 / 4000: loss 2.106907\n",
      "iteration 2500 / 4000: loss 1.854319\n",
      "iteration 2600 / 4000: loss 1.976305\n",
      "iteration 2700 / 4000: loss 2.089670\n",
      "iteration 2800 / 4000: loss 1.884366\n",
      "iteration 2900 / 4000: loss 1.972464\n",
      "iteration 3000 / 4000: loss 1.951530\n",
      "iteration 3100 / 4000: loss 2.112947\n",
      "iteration 3200 / 4000: loss 1.801674\n",
      "iteration 3300 / 4000: loss 1.730400\n",
      "iteration 3400 / 4000: loss 1.828392\n",
      "iteration 3500 / 4000: loss 2.114845\n",
      "iteration 3600 / 4000: loss 2.086144\n",
      "iteration 3700 / 4000: loss 2.087609\n",
      "iteration 3800 / 4000: loss 1.793139\n",
      "iteration 3900 / 4000: loss 1.808429\n",
      "iteration 0 / 4000: loss 4.000611\n",
      "iteration 100 / 4000: loss 2.867993\n",
      "iteration 200 / 4000: loss 2.526510\n",
      "iteration 300 / 4000: loss 2.173430\n",
      "iteration 400 / 4000: loss 2.445045\n",
      "iteration 500 / 4000: loss 1.974927\n",
      "iteration 600 / 4000: loss 1.995445\n",
      "iteration 700 / 4000: loss 1.737851\n",
      "iteration 800 / 4000: loss 2.072817\n",
      "iteration 900 / 4000: loss 1.962164\n",
      "iteration 1000 / 4000: loss 2.171003\n",
      "iteration 1100 / 4000: loss 2.191127\n",
      "iteration 1200 / 4000: loss 2.420408\n",
      "iteration 1300 / 4000: loss 1.870967\n",
      "iteration 1400 / 4000: loss 2.143380\n",
      "iteration 1500 / 4000: loss 1.837542\n",
      "iteration 1600 / 4000: loss 1.737261\n",
      "iteration 1700 / 4000: loss 2.226769\n",
      "iteration 1800 / 4000: loss 2.074933\n",
      "iteration 1900 / 4000: loss 1.931371\n",
      "iteration 2000 / 4000: loss 2.172609\n",
      "iteration 2100 / 4000: loss 2.066131\n",
      "iteration 2200 / 4000: loss 1.804736\n",
      "iteration 2300 / 4000: loss 2.051450\n",
      "iteration 2400 / 4000: loss 2.136465\n",
      "iteration 2500 / 4000: loss 2.244837\n",
      "iteration 2600 / 4000: loss 2.258405\n",
      "iteration 2700 / 4000: loss 2.101733\n",
      "iteration 2800 / 4000: loss 1.982909\n",
      "iteration 2900 / 4000: loss 1.954190\n",
      "iteration 3000 / 4000: loss 2.038623\n",
      "iteration 3100 / 4000: loss 1.929450\n",
      "iteration 3200 / 4000: loss 2.228333\n",
      "iteration 3300 / 4000: loss 2.064349\n",
      "iteration 3400 / 4000: loss 1.639878\n",
      "iteration 3500 / 4000: loss 1.944497\n",
      "iteration 3600 / 4000: loss 1.910182\n",
      "iteration 3700 / 4000: loss 2.068755\n",
      "iteration 3800 / 4000: loss 2.187461\n",
      "iteration 3900 / 4000: loss 2.102554\n",
      "iteration 0 / 4000: loss 3.999425\n",
      "iteration 100 / 4000: loss 3.698687\n",
      "iteration 200 / 4000: loss 3.385697\n",
      "iteration 300 / 4000: loss 3.010734\n",
      "iteration 400 / 4000: loss 2.403188\n",
      "iteration 500 / 4000: loss 2.421091\n",
      "iteration 600 / 4000: loss 2.464728\n",
      "iteration 700 / 4000: loss 2.524456\n",
      "iteration 800 / 4000: loss 1.948357\n",
      "iteration 900 / 4000: loss 2.506998\n",
      "iteration 1000 / 4000: loss 2.216715\n",
      "iteration 1100 / 4000: loss 2.250956\n",
      "iteration 1200 / 4000: loss 2.015896\n",
      "iteration 1300 / 4000: loss 2.370057\n",
      "iteration 1400 / 4000: loss 2.361788\n",
      "iteration 1500 / 4000: loss 2.058258\n",
      "iteration 1600 / 4000: loss 2.210110\n",
      "iteration 1700 / 4000: loss 2.296209\n",
      "iteration 1800 / 4000: loss 1.787560\n",
      "iteration 1900 / 4000: loss 1.930393\n",
      "iteration 2000 / 4000: loss 2.155590\n",
      "iteration 2100 / 4000: loss 2.024455\n",
      "iteration 2200 / 4000: loss 2.049658\n",
      "iteration 2300 / 4000: loss 2.310745\n",
      "iteration 2400 / 4000: loss 1.949093\n",
      "iteration 2500 / 4000: loss 2.015797\n",
      "iteration 2600 / 4000: loss 1.843144\n",
      "iteration 2700 / 4000: loss 2.288822\n",
      "iteration 2800 / 4000: loss 2.004878\n",
      "iteration 2900 / 4000: loss 2.038827\n",
      "iteration 3000 / 4000: loss 2.259596\n",
      "iteration 3100 / 4000: loss 2.226939\n",
      "iteration 3200 / 4000: loss 1.782591\n",
      "iteration 3300 / 4000: loss 2.211902\n",
      "iteration 3400 / 4000: loss 1.687491\n",
      "iteration 3500 / 4000: loss 2.140666\n",
      "iteration 3600 / 4000: loss 2.094794\n",
      "iteration 3700 / 4000: loss 1.892919\n",
      "iteration 3800 / 4000: loss 1.793982\n",
      "iteration 3900 / 4000: loss 1.758271\n",
      "iteration 0 / 4000: loss 4.000816\n",
      "iteration 100 / 4000: loss 1.706939\n",
      "iteration 200 / 4000: loss 1.805107\n",
      "iteration 300 / 4000: loss 1.980810\n",
      "iteration 400 / 4000: loss 1.969067\n",
      "iteration 500 / 4000: loss 1.916686\n",
      "iteration 600 / 4000: loss 1.911130\n",
      "iteration 700 / 4000: loss 1.721332\n",
      "iteration 800 / 4000: loss 1.690534\n",
      "iteration 900 / 4000: loss 1.725785\n",
      "iteration 1000 / 4000: loss 1.814262\n",
      "iteration 1100 / 4000: loss 1.709631\n",
      "iteration 1200 / 4000: loss 1.710573\n",
      "iteration 1300 / 4000: loss 1.619276\n",
      "iteration 1400 / 4000: loss 1.640647\n",
      "iteration 1500 / 4000: loss 1.892759\n",
      "iteration 1600 / 4000: loss 1.654416\n",
      "iteration 1700 / 4000: loss 1.692332\n",
      "iteration 1800 / 4000: loss 1.749877\n",
      "iteration 1900 / 4000: loss 1.762546\n",
      "iteration 2000 / 4000: loss 1.616296\n",
      "iteration 2100 / 4000: loss 1.507089\n",
      "iteration 2200 / 4000: loss 1.692781\n",
      "iteration 2300 / 4000: loss 1.771417\n",
      "iteration 2400 / 4000: loss 1.653593\n",
      "iteration 2500 / 4000: loss 1.451461\n",
      "iteration 2600 / 4000: loss 1.764569\n",
      "iteration 2700 / 4000: loss 1.550086\n",
      "iteration 2800 / 4000: loss 1.548528\n",
      "iteration 2900 / 4000: loss 1.767433\n",
      "iteration 3000 / 4000: loss 1.767905\n",
      "iteration 3100 / 4000: loss 1.570007\n",
      "iteration 3200 / 4000: loss 1.506061\n",
      "iteration 3300 / 4000: loss 1.633822\n",
      "iteration 3400 / 4000: loss 1.713032\n",
      "iteration 3500 / 4000: loss 1.701721\n",
      "iteration 3600 / 4000: loss 1.606787\n",
      "iteration 3700 / 4000: loss 1.521351\n",
      "iteration 3800 / 4000: loss 1.626762\n",
      "iteration 3900 / 4000: loss 1.749020\n",
      "iteration 0 / 4000: loss 3.997319\n",
      "iteration 100 / 4000: loss 2.234265\n",
      "iteration 200 / 4000: loss 2.231144\n",
      "iteration 300 / 4000: loss 2.087432\n",
      "iteration 400 / 4000: loss 1.828176\n",
      "iteration 500 / 4000: loss 1.965819\n",
      "iteration 600 / 4000: loss 1.922569\n",
      "iteration 700 / 4000: loss 1.779941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 4000: loss 2.128423\n",
      "iteration 900 / 4000: loss 1.713053\n",
      "iteration 1000 / 4000: loss 1.796730\n",
      "iteration 1100 / 4000: loss 1.869494\n",
      "iteration 1200 / 4000: loss 1.542984\n",
      "iteration 1300 / 4000: loss 1.741992\n",
      "iteration 1400 / 4000: loss 1.752513\n",
      "iteration 1500 / 4000: loss 1.749299\n",
      "iteration 1600 / 4000: loss 1.694660\n",
      "iteration 1700 / 4000: loss 2.012653\n",
      "iteration 1800 / 4000: loss 1.845588\n",
      "iteration 1900 / 4000: loss 2.028444\n",
      "iteration 2000 / 4000: loss 1.800285\n",
      "iteration 2100 / 4000: loss 2.108757\n",
      "iteration 2200 / 4000: loss 1.672702\n",
      "iteration 2300 / 4000: loss 1.715146\n",
      "iteration 2400 / 4000: loss 2.008560\n",
      "iteration 2500 / 4000: loss 1.636850\n",
      "iteration 2600 / 4000: loss 1.598573\n",
      "iteration 2700 / 4000: loss 1.573128\n",
      "iteration 2800 / 4000: loss 1.659121\n",
      "iteration 2900 / 4000: loss 1.533163\n",
      "iteration 3000 / 4000: loss 1.810785\n",
      "iteration 3100 / 4000: loss 1.753007\n",
      "iteration 3200 / 4000: loss 1.827065\n",
      "iteration 3300 / 4000: loss 1.672335\n",
      "iteration 3400 / 4000: loss 1.825791\n",
      "iteration 3500 / 4000: loss 1.799044\n",
      "iteration 3600 / 4000: loss 1.998033\n",
      "iteration 3700 / 4000: loss 1.912105\n",
      "iteration 3800 / 4000: loss 1.657121\n",
      "iteration 3900 / 4000: loss 1.630266\n",
      "iteration 0 / 4000: loss 3.995388\n",
      "iteration 100 / 4000: loss 2.183261\n",
      "iteration 200 / 4000: loss 2.277181\n",
      "iteration 300 / 4000: loss 1.856805\n",
      "iteration 400 / 4000: loss 2.016186\n",
      "iteration 500 / 4000: loss 1.887779\n",
      "iteration 600 / 4000: loss 1.843305\n",
      "iteration 700 / 4000: loss 1.780426\n",
      "iteration 800 / 4000: loss 1.964259\n",
      "iteration 900 / 4000: loss 1.883837\n",
      "iteration 1000 / 4000: loss 1.879363\n",
      "iteration 1100 / 4000: loss 1.962758\n",
      "iteration 1200 / 4000: loss 1.960645\n",
      "iteration 1300 / 4000: loss 1.870805\n",
      "iteration 1400 / 4000: loss 1.927180\n",
      "iteration 1500 / 4000: loss 1.760139\n",
      "iteration 1600 / 4000: loss 1.997219\n",
      "iteration 1700 / 4000: loss 1.673739\n",
      "iteration 1800 / 4000: loss 1.800077\n",
      "iteration 1900 / 4000: loss 2.128310\n",
      "iteration 2000 / 4000: loss 2.019733\n",
      "iteration 2100 / 4000: loss 2.155425\n",
      "iteration 2200 / 4000: loss 1.838990\n",
      "iteration 2300 / 4000: loss 1.820385\n",
      "iteration 2400 / 4000: loss 1.937189\n",
      "iteration 2500 / 4000: loss 1.653970\n",
      "iteration 2600 / 4000: loss 2.177505\n",
      "iteration 2700 / 4000: loss 1.739215\n",
      "iteration 2800 / 4000: loss 2.130548\n",
      "iteration 2900 / 4000: loss 1.564830\n",
      "iteration 3000 / 4000: loss 1.923526\n",
      "iteration 3100 / 4000: loss 1.685684\n",
      "iteration 3200 / 4000: loss 1.907628\n",
      "iteration 3300 / 4000: loss 2.040226\n",
      "iteration 3400 / 4000: loss 1.933812\n",
      "iteration 3500 / 4000: loss 1.908987\n",
      "iteration 3600 / 4000: loss 1.829404\n",
      "iteration 3700 / 4000: loss 1.801691\n",
      "iteration 3800 / 4000: loss 2.023648\n",
      "iteration 3900 / 4000: loss 1.919153\n",
      "iteration 0 / 4000: loss 3.997260\n",
      "iteration 100 / 4000: loss 3.007608\n",
      "iteration 200 / 4000: loss 2.387819\n",
      "iteration 300 / 4000: loss 2.188407\n",
      "iteration 400 / 4000: loss 2.192594\n",
      "iteration 500 / 4000: loss 1.944161\n",
      "iteration 600 / 4000: loss 2.285884\n",
      "iteration 700 / 4000: loss 2.157547\n",
      "iteration 800 / 4000: loss 2.045498\n",
      "iteration 900 / 4000: loss 1.891498\n",
      "iteration 1000 / 4000: loss 1.944140\n",
      "iteration 1100 / 4000: loss 1.948047\n",
      "iteration 1200 / 4000: loss 1.890155\n",
      "iteration 1300 / 4000: loss 2.228048\n",
      "iteration 1400 / 4000: loss 1.911274\n",
      "iteration 1500 / 4000: loss 2.113358\n",
      "iteration 1600 / 4000: loss 2.015985\n",
      "iteration 1700 / 4000: loss 2.035617\n",
      "iteration 1800 / 4000: loss 1.872747\n",
      "iteration 1900 / 4000: loss 1.891155\n",
      "iteration 2000 / 4000: loss 2.133583\n",
      "iteration 2100 / 4000: loss 1.896040\n",
      "iteration 2200 / 4000: loss 2.065833\n",
      "iteration 2300 / 4000: loss 2.134367\n",
      "iteration 2400 / 4000: loss 1.827761\n",
      "iteration 2500 / 4000: loss 2.347917\n",
      "iteration 2600 / 4000: loss 1.675608\n",
      "iteration 2700 / 4000: loss 1.840071\n",
      "iteration 2800 / 4000: loss 2.074059\n",
      "iteration 2900 / 4000: loss 1.968310\n",
      "iteration 3000 / 4000: loss 2.085356\n",
      "iteration 3100 / 4000: loss 1.874415\n",
      "iteration 3200 / 4000: loss 2.117404\n",
      "iteration 3300 / 4000: loss 1.920478\n",
      "iteration 3400 / 4000: loss 1.792808\n",
      "iteration 3500 / 4000: loss 1.825272\n",
      "iteration 3600 / 4000: loss 1.747893\n",
      "iteration 3700 / 4000: loss 1.737967\n",
      "iteration 3800 / 4000: loss 1.791075\n",
      "iteration 3900 / 4000: loss 2.274231\n",
      "iteration 0 / 4000: loss 4.003316\n",
      "iteration 100 / 4000: loss 3.573131\n",
      "iteration 200 / 4000: loss 3.169751\n",
      "iteration 300 / 4000: loss 2.836940\n",
      "iteration 400 / 4000: loss 2.578505\n",
      "iteration 500 / 4000: loss 2.429068\n",
      "iteration 600 / 4000: loss 2.403359\n",
      "iteration 700 / 4000: loss 2.282327\n",
      "iteration 800 / 4000: loss 2.271763\n",
      "iteration 900 / 4000: loss 2.524917\n",
      "iteration 1000 / 4000: loss 2.245205\n",
      "iteration 1100 / 4000: loss 2.362152\n",
      "iteration 1200 / 4000: loss 1.966234\n",
      "iteration 1300 / 4000: loss 2.120336\n",
      "iteration 1400 / 4000: loss 2.329303\n",
      "iteration 1500 / 4000: loss 2.163508\n",
      "iteration 1600 / 4000: loss 2.192112\n",
      "iteration 1700 / 4000: loss 1.958725\n",
      "iteration 1800 / 4000: loss 2.355894\n",
      "iteration 1900 / 4000: loss 1.966007\n",
      "iteration 2000 / 4000: loss 2.223714\n",
      "iteration 2100 / 4000: loss 2.003862\n",
      "iteration 2200 / 4000: loss 1.948176\n",
      "iteration 2300 / 4000: loss 2.047585\n",
      "iteration 2400 / 4000: loss 2.296161\n",
      "iteration 2500 / 4000: loss 2.203073\n",
      "iteration 2600 / 4000: loss 1.901453\n",
      "iteration 2700 / 4000: loss 2.100938\n",
      "iteration 2800 / 4000: loss 2.377094\n",
      "iteration 2900 / 4000: loss 1.754822\n",
      "iteration 3000 / 4000: loss 2.275689\n",
      "iteration 3100 / 4000: loss 2.105337\n",
      "iteration 3200 / 4000: loss 2.332098\n",
      "iteration 3300 / 4000: loss 2.155790\n",
      "iteration 3400 / 4000: loss 2.224893\n",
      "iteration 3500 / 4000: loss 1.996785\n",
      "iteration 3600 / 4000: loss 1.820432\n",
      "iteration 3700 / 4000: loss 2.027957\n",
      "iteration 3800 / 4000: loss 2.147241\n",
      "iteration 3900 / 4000: loss 2.025099\n",
      "iteration 0 / 4000: loss 4.001624\n",
      "iteration 100 / 4000: loss 2.134138\n",
      "iteration 200 / 4000: loss 1.659742\n",
      "iteration 300 / 4000: loss 1.812403\n",
      "iteration 400 / 4000: loss 2.026475\n",
      "iteration 500 / 4000: loss 1.708525\n",
      "iteration 600 / 4000: loss 1.660252\n",
      "iteration 700 / 4000: loss 1.849972\n",
      "iteration 800 / 4000: loss 1.810412\n",
      "iteration 900 / 4000: loss 1.897369\n",
      "iteration 1000 / 4000: loss 1.758961\n",
      "iteration 1100 / 4000: loss 1.898345\n",
      "iteration 1200 / 4000: loss 1.838337\n",
      "iteration 1300 / 4000: loss 1.790620\n",
      "iteration 1400 / 4000: loss 1.741355\n",
      "iteration 1500 / 4000: loss 1.589223\n",
      "iteration 1600 / 4000: loss 1.642944\n",
      "iteration 1700 / 4000: loss 1.453264\n",
      "iteration 1800 / 4000: loss 1.536654\n",
      "iteration 1900 / 4000: loss 1.575387\n",
      "iteration 2000 / 4000: loss 1.546788\n",
      "iteration 2100 / 4000: loss 1.773814\n",
      "iteration 2200 / 4000: loss 1.744828\n",
      "iteration 2300 / 4000: loss 1.582237\n",
      "iteration 2400 / 4000: loss 1.509333\n",
      "iteration 2500 / 4000: loss 1.598204\n",
      "iteration 2600 / 4000: loss 1.764958\n",
      "iteration 2700 / 4000: loss 1.650964\n",
      "iteration 2800 / 4000: loss 1.661580\n",
      "iteration 2900 / 4000: loss 1.835411\n",
      "iteration 3000 / 4000: loss 1.632115\n",
      "iteration 3100 / 4000: loss 1.419673\n",
      "iteration 3200 / 4000: loss 1.386555\n",
      "iteration 3300 / 4000: loss 1.610955\n",
      "iteration 3400 / 4000: loss 1.751584\n",
      "iteration 3500 / 4000: loss 1.634214\n",
      "iteration 3600 / 4000: loss 1.726069\n",
      "iteration 3700 / 4000: loss 1.386192\n",
      "iteration 3800 / 4000: loss 1.683178\n",
      "iteration 3900 / 4000: loss 1.704267\n",
      "iteration 0 / 4000: loss 4.002804\n",
      "iteration 100 / 4000: loss 1.815797\n",
      "iteration 200 / 4000: loss 2.002997\n",
      "iteration 300 / 4000: loss 1.761675\n",
      "iteration 400 / 4000: loss 1.818429\n",
      "iteration 500 / 4000: loss 1.814646\n",
      "iteration 600 / 4000: loss 2.022576\n",
      "iteration 700 / 4000: loss 1.919899\n",
      "iteration 800 / 4000: loss 1.829695\n",
      "iteration 900 / 4000: loss 1.887517\n",
      "iteration 1000 / 4000: loss 1.924922\n",
      "iteration 1100 / 4000: loss 1.635631\n",
      "iteration 1200 / 4000: loss 1.798280\n",
      "iteration 1300 / 4000: loss 1.680332\n",
      "iteration 1400 / 4000: loss 1.739294\n",
      "iteration 1500 / 4000: loss 1.715896\n",
      "iteration 1600 / 4000: loss 1.700469\n",
      "iteration 1700 / 4000: loss 1.711900\n",
      "iteration 1800 / 4000: loss 1.625082\n",
      "iteration 1900 / 4000: loss 1.958163\n",
      "iteration 2000 / 4000: loss 1.382095\n",
      "iteration 2100 / 4000: loss 1.813429\n",
      "iteration 2200 / 4000: loss 1.717719\n",
      "iteration 2300 / 4000: loss 1.533864\n",
      "iteration 2400 / 4000: loss 1.767867\n",
      "iteration 2500 / 4000: loss 1.954807\n",
      "iteration 2600 / 4000: loss 1.613599\n",
      "iteration 2700 / 4000: loss 2.097673\n",
      "iteration 2800 / 4000: loss 1.678593\n",
      "iteration 2900 / 4000: loss 1.966587\n",
      "iteration 3000 / 4000: loss 1.938136\n",
      "iteration 3100 / 4000: loss 1.832847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200 / 4000: loss 2.084035\n",
      "iteration 3300 / 4000: loss 1.622719\n",
      "iteration 3400 / 4000: loss 1.508672\n",
      "iteration 3500 / 4000: loss 1.628266\n",
      "iteration 3600 / 4000: loss 1.798742\n",
      "iteration 3700 / 4000: loss 2.025379\n",
      "iteration 3800 / 4000: loss 1.691906\n",
      "iteration 3900 / 4000: loss 1.707239\n",
      "iteration 0 / 4000: loss 4.002244\n",
      "iteration 100 / 4000: loss 2.129398\n",
      "iteration 200 / 4000: loss 2.088004\n",
      "iteration 300 / 4000: loss 2.123487\n",
      "iteration 400 / 4000: loss 2.078074\n",
      "iteration 500 / 4000: loss 1.865843\n",
      "iteration 600 / 4000: loss 1.737386\n",
      "iteration 700 / 4000: loss 2.060316\n",
      "iteration 800 / 4000: loss 2.038479\n",
      "iteration 900 / 4000: loss 2.032590\n",
      "iteration 1000 / 4000: loss 1.945288\n",
      "iteration 1100 / 4000: loss 2.131102\n",
      "iteration 1200 / 4000: loss 1.831093\n",
      "iteration 1300 / 4000: loss 1.994791\n",
      "iteration 1400 / 4000: loss 2.086016\n",
      "iteration 1500 / 4000: loss 2.167119\n",
      "iteration 1600 / 4000: loss 1.798636\n",
      "iteration 1700 / 4000: loss 1.923999\n",
      "iteration 1800 / 4000: loss 1.891363\n",
      "iteration 1900 / 4000: loss 2.271803\n",
      "iteration 2000 / 4000: loss 1.979645\n",
      "iteration 2100 / 4000: loss 1.879020\n",
      "iteration 2200 / 4000: loss 1.696000\n",
      "iteration 2300 / 4000: loss 1.712232\n",
      "iteration 2400 / 4000: loss 1.958260\n",
      "iteration 2500 / 4000: loss 1.823198\n",
      "iteration 2600 / 4000: loss 1.868078\n",
      "iteration 2700 / 4000: loss 1.638311\n",
      "iteration 2800 / 4000: loss 1.559810\n",
      "iteration 2900 / 4000: loss 1.585059\n",
      "iteration 3000 / 4000: loss 1.707556\n",
      "iteration 3100 / 4000: loss 2.031167\n",
      "iteration 3200 / 4000: loss 1.943813\n",
      "iteration 3300 / 4000: loss 1.760047\n",
      "iteration 3400 / 4000: loss 1.948934\n",
      "iteration 3500 / 4000: loss 1.883442\n",
      "iteration 3600 / 4000: loss 1.729532\n",
      "iteration 3700 / 4000: loss 1.722518\n",
      "iteration 3800 / 4000: loss 1.753953\n",
      "iteration 3900 / 4000: loss 2.026302\n",
      "iteration 0 / 4000: loss 3.999910\n",
      "iteration 100 / 4000: loss 2.964129\n",
      "iteration 200 / 4000: loss 2.302024\n",
      "iteration 300 / 4000: loss 2.265643\n",
      "iteration 400 / 4000: loss 2.037966\n",
      "iteration 500 / 4000: loss 2.276957\n",
      "iteration 600 / 4000: loss 2.193548\n",
      "iteration 700 / 4000: loss 1.890932\n",
      "iteration 800 / 4000: loss 1.865020\n",
      "iteration 900 / 4000: loss 2.312332\n",
      "iteration 1000 / 4000: loss 2.252159\n",
      "iteration 1100 / 4000: loss 1.976505\n",
      "iteration 1200 / 4000: loss 1.964460\n",
      "iteration 1300 / 4000: loss 2.090249\n",
      "iteration 1400 / 4000: loss 1.844464\n",
      "iteration 1500 / 4000: loss 2.294467\n",
      "iteration 1600 / 4000: loss 2.109950\n",
      "iteration 1700 / 4000: loss 1.924741\n",
      "iteration 1800 / 4000: loss 1.686069\n",
      "iteration 1900 / 4000: loss 1.850256\n",
      "iteration 2000 / 4000: loss 1.982094\n",
      "iteration 2100 / 4000: loss 1.856016\n",
      "iteration 2200 / 4000: loss 2.001290\n",
      "iteration 2300 / 4000: loss 1.696036\n",
      "iteration 2400 / 4000: loss 1.970316\n",
      "iteration 2500 / 4000: loss 2.066398\n",
      "iteration 2600 / 4000: loss 1.961417\n",
      "iteration 2700 / 4000: loss 1.847382\n",
      "iteration 2800 / 4000: loss 1.965245\n",
      "iteration 2900 / 4000: loss 1.837485\n",
      "iteration 3000 / 4000: loss 1.843983\n",
      "iteration 3100 / 4000: loss 1.877259\n",
      "iteration 3200 / 4000: loss 2.013198\n",
      "iteration 3300 / 4000: loss 2.147612\n",
      "iteration 3400 / 4000: loss 2.158516\n",
      "iteration 3500 / 4000: loss 1.829636\n",
      "iteration 3600 / 4000: loss 1.891967\n",
      "iteration 3700 / 4000: loss 2.259499\n",
      "iteration 3800 / 4000: loss 1.897781\n",
      "iteration 3900 / 4000: loss 1.918252\n",
      "iteration 0 / 4000: loss 3.999882\n",
      "iteration 100 / 4000: loss 3.640085\n",
      "iteration 200 / 4000: loss 3.281715\n",
      "iteration 300 / 4000: loss 2.800111\n",
      "iteration 400 / 4000: loss 2.620597\n",
      "iteration 500 / 4000: loss 2.390307\n",
      "iteration 600 / 4000: loss 2.323923\n",
      "iteration 700 / 4000: loss 2.445527\n",
      "iteration 800 / 4000: loss 2.190130\n",
      "iteration 900 / 4000: loss 2.129606\n",
      "iteration 1000 / 4000: loss 1.931731\n",
      "iteration 1100 / 4000: loss 2.261593\n",
      "iteration 1200 / 4000: loss 2.221427\n",
      "iteration 1300 / 4000: loss 2.321618\n",
      "iteration 1400 / 4000: loss 2.287421\n",
      "iteration 1500 / 4000: loss 2.162431\n",
      "iteration 1600 / 4000: loss 1.881547\n",
      "iteration 1700 / 4000: loss 2.095866\n",
      "iteration 1800 / 4000: loss 2.141315\n",
      "iteration 1900 / 4000: loss 1.995273\n",
      "iteration 2000 / 4000: loss 1.874014\n",
      "iteration 2100 / 4000: loss 2.223077\n",
      "iteration 2200 / 4000: loss 2.275613\n",
      "iteration 2300 / 4000: loss 2.223764\n",
      "iteration 2400 / 4000: loss 1.937217\n",
      "iteration 2500 / 4000: loss 2.107713\n",
      "iteration 2600 / 4000: loss 2.057512\n",
      "iteration 2700 / 4000: loss 2.098387\n",
      "iteration 2800 / 4000: loss 2.183209\n",
      "iteration 2900 / 4000: loss 1.937016\n",
      "iteration 3000 / 4000: loss 2.069578\n",
      "iteration 3100 / 4000: loss 2.025645\n",
      "iteration 3200 / 4000: loss 1.641137\n",
      "iteration 3300 / 4000: loss 2.267293\n",
      "iteration 3400 / 4000: loss 2.046036\n",
      "iteration 3500 / 4000: loss 2.011382\n",
      "iteration 3600 / 4000: loss 1.711436\n",
      "iteration 3700 / 4000: loss 1.729184\n",
      "iteration 3800 / 4000: loss 2.136605\n",
      "iteration 3900 / 4000: loss 2.141994\n"
     ]
    }
   ],
   "source": [
    "from svm import SVM\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_svm = None\n",
    "learning_rates = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3]\n",
    "features = [0,1,2,3]\n",
    "\n",
    "\n",
    "for i in features:\n",
    "    X_train = np.copy(train_text)\n",
    "    X_val = np.copy(val_text)\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else: \n",
    "        X_train = np.delete(train_text,list(range(1000*(i-1),1000*i)),axis = 1)\n",
    "        X_val = np.delete(val_text,list(range(1000*(i-1),1000*i)), axis = 1)\n",
    "        \n",
    "    #添加bias\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    for lr in learning_rates:\n",
    "        model = SVM()\n",
    "        \n",
    "        model.train(X_train, y_train, learning_rate = lr, num_iters = 4000)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        acc_train = np.mean(y_pred_train == y_train)\n",
    "        \n",
    "        y_pred_val = model.predict(X_val)\n",
    "        acc_val = np.mean(y_pred_val == y_val)\n",
    "        results[(lr, i)] = (acc_train, acc_val)\n",
    "        \n",
    "        if acc_val > best_val:\n",
    "            best_val = acc_val\n",
    "            best_svm = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved during validation: 0.567746\n"
     ]
    }
   ],
   "source": [
    "svm_df = pd.DataFrame(np.zeros((4,5)),index = ['N1,N2,N3','N2,N3','N1,N3','N1,N2'],columns = learning_rates)\n",
    "feature_dict = {\n",
    "    0:'N1,N2,N3',\n",
    "    1:'N2,N3',\n",
    "    2:'N1,N3',\n",
    "    3:'N1,N2'\n",
    "}\n",
    "for lr, gram in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, gram)]\n",
    "    svm_df[lr][feature_dict[gram]] = val_accuracy\n",
    "    \n",
    "print('best validation accuracy achieved during validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.03</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.003</th>\n",
       "      <th>0.001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N1,N2,N3</th>\n",
       "      <td>0.567746</td>\n",
       "      <td>0.543960</td>\n",
       "      <td>0.526588</td>\n",
       "      <td>0.510523</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N2,N3</th>\n",
       "      <td>0.525280</td>\n",
       "      <td>0.514633</td>\n",
       "      <td>0.510149</td>\n",
       "      <td>0.510212</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1,N3</th>\n",
       "      <td>0.563823</td>\n",
       "      <td>0.541096</td>\n",
       "      <td>0.523412</td>\n",
       "      <td>0.510212</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N1,N2</th>\n",
       "      <td>0.565131</td>\n",
       "      <td>0.541594</td>\n",
       "      <td>0.526961</td>\n",
       "      <td>0.510274</td>\n",
       "      <td>0.510212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0.100     0.030     0.010     0.003     0.001\n",
       "N1,N2,N3  0.567746  0.543960  0.526588  0.510523  0.510212\n",
       "N2,N3     0.525280  0.514633  0.510149  0.510212  0.510212\n",
       "N1,N3     0.563823  0.541096  0.523412  0.510212  0.510212\n",
       "N1,N2     0.565131  0.541594  0.526961  0.510274  0.510212"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.copy(test_text)\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "test_predictions = best_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\n",
    "    'PhraseId': test['PhraseId'],\n",
    "    'Sentiment': test_predictions\n",
    "})\n",
    "\n",
    "output.to_csv('svm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上传至kaggle后，得分为0.56533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
